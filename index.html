<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="掌握java相关知识，擅长大数据，以及机器学习算法应用方面 java scala python"><meta name="keywords" content><meta name="author" content="zhuyuping"><meta name="copyright" content="zhuyuping"><title>朱遇平的github博客</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.0"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">zhuyuping</div><div class="author-info__description text-center">掌握java相关知识，擅长大数据，以及机器学习算法应用方面 java scala python</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">19</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">4</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">朱遇平的github博客</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">朱遇平的github博客</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/29/ddd/">我对DDD领域驱动设计的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-29</time><div class="content"><p>日常我们在做领域设计时候，我的经验是需要掌握4+1<br>限定上下文<br>空间<br>边界<br>行为<br>核心<br>最后统一<br>而每个部分根据层次分类，可以组成更细的单<br>这是不是很像生活中的某些事情呢？，存在就有道理，为啥这么组织协调，就是因为这些划分组织结构带来的优越性。<br>其实DDD特别像从隋炀帝开始设立的三省六部值。到今天很像国家组织结构划分，划分相应的部门，每个部门的职能，边界定义，上下文限定，而每个部门下面有更小层次的子部门，还存在相关统筹部门 像聚合根，中央制定政策，下面统筹部门相应的安排相关人员去负责相应的事务，而每个部门对外有相应的接口门面为老百姓服务。其实DDD最重要的是业务的边界划分，如果你能够找到准确的业务边界划分，那么DDD就是很简单了，而怎么找到业务边界划分呢？这个需要业务专家以及业务理解认识，更重要的是有一些方法的，想象小时候 我们看了很多文章，怎么找到散文中所表达的关键主线思想，那么领域驱动核心域以及边界定义通用的方式，就可以准确的找到的相应的域，最后就是业务的设计模式构建了。整个过程对我们来说就是一个从业务架构到领域设计到系统架构到基础架构的过程。其实基础架构同样可以用领域DDD建模设计的，比如K8S内部架构真的很相似哦，甚至内部类的命名实现。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/23/选举算法/">有意思的选举算法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-23</time><div class="content"><p>看了很多理论，很多知识都慢慢结构化思维了，<br>后面还是慢慢填充扩展，深入。今天讲一下选举算法<br>选举算法在很多时候都要使用到她，比如读写分离，比如任务并行化，比如副本同步，选举出主节点，可以减少并发也可以把问题复杂度大大减少。在zookeeper 在raft 在网络拓扑图，甚至一些锁处理，一些时间戳，id分配时候等等都会涉及到分布式选举算法。</p>
<p>常用的分布式选举算法就是通过一定的规则破坏相互之间的对称性，通过部分不平衡或者优先级选举出协调者。</p>
<p><strong>常用的 选举算法，大多实现通过优先级或者id全定序或者T时间来破坏这对称性。破坏了对称性之后选择max min的节点作为leader</strong></p>
<p><strong>分布式环算法</strong>：<br>    原理就是环形接力比赛，如果传到正确的人，这人当选leader，如果没有，传递给下一个人。<br>这里是可以通过多钟方式破坏平衡性，比如消息id或者优先级比自己大的传递给下一个人，如果消息比自己小的把消息id或者优先级替换成自己的。还有通过令牌等等。<br><strong>欺负bully算法</strong>：<br>   原理是只给优先级或者进程id比自己高的人发送投票，看看有没有比我大的人，在就乖乖的做follow 不在就当选老大，如果老大在，老大收到了就回复。 所以叫做欺负算法。这里也是通过选大的规则破坏平衡对称性这样才能产生leader.<br>这种方式跟raft 以及 zab选举类似，接收到大的消息进行变更自己消息并进行回复。<br>比如还有通过<strong>超时心跳时间</strong>来选举的。不断的发送心跳周期T，如果最近2T时间没有收到比自己编号大的心跳消息时候自己宣威leader</p>
<p><strong>那么是不是一定要通过破坏平衡性方式来选举leader呢？？，不是的，只是破坏对称性对于分布式系统来说是一种常用的方式。比如租约lease方式，赋予一段时期给与一个权力，做leader操作相关指令的许可。这个在hdfs hbase中都有使用到它，简单有效，特别是读多写少的场景</strong>。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/19/LSM树的反思/">一些架构设计常接触的思想结构</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-19</time><div class="content"><p>LSM树的反思：<br>最近复习了一下LSM树，发现自己以前认识的不够深刻，从接触NoSQL数据库开始，就不得不接触到LSM结构，从Hbase使用，levelDB等使用，一直觉得LSM是为了在内存排序预先好，比如Hbase 在内存中进行写入LSM树，维持一些key的顺序，然后同时不断的进行三层合并，写入到HFile B+树结构，但是，没有认识到深层次的原因，其实LSM个人认为是一种思想吧，通过合并树，定时在在内存维持着一个有顺序的机构，可以是hashmap可以是红黑树，可以是skiplist，可以使B树，当达到一定阈值时候，不断的与磁盘中的子树进行合并，其实这样做为了写入的性能，能够把随机写，变成有顺序的写，比如内存的LSM的skiplist 可以写入到磁盘中变成一系列有顺序的key 组成的文件列表，最后可以通过compact进行顺序的key区间合并。所以LSM应该是一种思想，牺牲了一定的读性能，是随机写变成了有顺序写，大大提高了磁盘写性能。更方便了后面进行文件的合并。比如leveldb就是这样做的。</p>
<p>Hash的反思：</p>
<p>说起hash结构，作为一种查找/索引的方式，常用的有bitset图思想，有hash查找，有radix查找，有布谷鸟hash等方式，其中我们常用boomfilter布隆过滤器做去重，因为他会误报但是不会漏报，也就是本来存在的一定会判断存在，也有kylin用它来做基数计数统计，为了支持添加删除会用多个位来进行表示相关属性，另外radix 基数，我们用来做前缀 后缀等匹配查找以及计数，正常的hash就是类似我们map方式。通过hash算法比如mumurhash2,然后映射到数组或类似快速查找结构上进行存储，但是因为有利用度不够以及扩容问题，所以有时候要进行rehash扩容处理。对于布谷鸟hash其实有点类似<br>LSH敏感方式，多个k hash方式，获取多个位置，进行链式反应，可以做到很高的空间利用率，但是本质上hash上损失了一定的性能，链式反应以及多个hash过程中。</p>
<p>其实hash不仅仅用于索引查找过程作用，更多时候是实现CAP 的P 进行多副本时候，通常有随机robin取模方式 还有一致性hash方式 还有range区域表管理方式。</p>
<p>一致性hash中，有很多种方式，常用的是DHT的chord类似的方式，一致性hash环，通过映射到环形结构中，顺时针查找相应的节点，如果节点过多，可以每个节点维护一种节点路由表，可以进行2的幂进行查找，logN 时间复杂度 ，查找相应的节点，无论是节点加入还是退出都可以只需要更新重新负载<br>后继节点就可以了，但是这在节点少时候会存在不平衡问题，所以后面就引入了虚拟group概念，减少这种热点不平衡性。</p>
<p>至于说是不是副本一定要用一致性hash是不是一定要用一致性hash这种方式更好呢，不是的，其实range方式也很常用，只需要实现key到partion映射，而partion维护在内存表中动态进行更改分片就行，比如hbase也是这样实现region的，还有hdfs的，多副本一致性，可以采用同步复制，异步复制，<br>混合方式，以及paxso raft方式，或者 NWR机制方式，本质上没有好坏。比如kafka是混合复制方式。但是最新版的开始打算采用raft方式了。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/02/diststore/">说说自己大数据接触的ES/HBase/TSDB/Redis/Celler</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-02</time><div class="content"><p>因为以前个人工作平台架构工作主要负责中间件与大数据层，很多存储比如ES 比如 Hbase 比如 TSDB 比如 Celler，其实还有data hive alluxio但是这些其实这是数据组织结构的变化，或者相关引擎，相关大数据内存加速技术，其实接触多了，了解其实现久了你就会发现其实他们设计思想原理都很相似或许一些特性的不同决定了他们的作用。</p>
<p> 从设计上来说 他们都有一个协调者负责进行schema metadata信息的维护管理，通过一致性算法或者zk或者db方式或者复制机制、quarum机制来实现多副本一致性以及存储的分片副本容灾以及路由数据获取，他们都有通用的套路WAL+log机制 快照副本，他们都有尝试着zerocopy DMA机制以及pageCache来加快存储，他们有LRU 或者boolmfilter来过滤一些无效请求或者key,其实这些都是为了更大吞吐量更快速度，分布式互斥资源并发考虑，比如还有一些租约lease机制，比如还有一些quarum NWR机制，比如还有一些MVCC机制，还有就是副本的一致性考虑的，也就是CAP基础上尽可能保证高速度高吞吐量。剩余的就是怎么索引，至于常用的索引技术，有Hash索引 k/v,有倒排，有b/b+ 有LSM结构,有LSH bitSet 等等，这些思想套路其实在很多数据库上都类似。就像分布式事务套路：TCC 2PC 补偿对账 去重重发表就那么几种一样。分库分表套路：索引表，复写避免读，先hash在range ,hash合并hash，分表索引计数等等几种套路。明白了这些思想这些套路为什么这么做，为什么只能这么做就可以了解的，其实分布式存储也是一样，所以ES/HBase/TSDB/Redis/Celler 一些思想手段都是一样的。后来有一类新数据newSQL为了支持事务，不仅仅结合了mysql的ARIES算法，也才去了逻辑时钟全定序思想排序事务id,结合mvcc 2pc来实现事务功能，大体来说这是我做大数据基础架构接触到所认识到的东西。<br> 下面分别逐一讲一下，行列数据库区别不讲了，网上很多，其实这不是重要的。<br> <strong>下面介绍ES：</strong><br> ES 来说其实最主要的是倒排索引，当我们说<br> 正常的K==&gt;V 是文章XX =》 文章内容时候， 倒排是对V进行分词提取 分别建立V1 ==&gt; K V2=&gt;K V3=&gt;K,这样可以通过部分关键字V1 进行搜索匹配。这部分es是通过lucene进行的，关键在于他不仅仅记录了这样的key 还会记录字符位置关键字位置，以及频率得分，这样可以进行排序以及快速定位位置。另外还有一些压缩技术，这些使用pageCache大大加快了速度。所以es设置内存大小时候应该合理设置。<br> <strong>下面介绍Hbase:</strong></p>
<p> Hbase 这些通过master维护region 通过zk维护hbase表metadata信息，通过wal+log进行追加MVCC多版本实现update del 后面进行小文件合并大文件合并从而实现适合HDFS的存储，以及移除一些不必要的版本数据。而memstore 以LSM树内存中维护这排序结构，不断的内存中合并成小HFile，每一个HFile其实是一个B+树结构，然后后台大合并<br> 最后合并成大的HFile存储在HDFS中，然后为了避免一些不必要的查询操作，建立了缓存boomfilter,其他的就是列式数据库，所以使用了rowKey划分一个个region，预分区，master管理这些region以及副本。大体就是这样的。</p>
<p> <strong>下面介绍TSDB:</strong> </p>
<p> 我在喜马拉雅进行Xdcs日志监控系统中使用了openTSDB 其实是搭建在我们hbase基础上的时间系列数据库，他的思想就是我上面说的套路也就是索引表，对于metries tagk tagv组合都使用一张索引表存储，同时给他们编号唯一的id组合，然后对于每个事件的记录，提取完整事件的时间戳，组合上面的索引表的id 形成唯一的rowkey 而剩下的时间段，比如1小时的3600s ，比如某个时间是该时间里面1800秒那么他会在列中以1800s作为列名进行存储。大体就是这样。后来我的项目为了方便使用我试着加上apache calite 添加SQL解析层方便tsdb使用。</p>
<p> <strong>下面介绍Redis Cellar:</strong></p>
<p> 我了解的redis其实并不多，因为他是C开发的，在使用中我们常用他实现分布式锁，redssion开源项目结合使用，使用lua脚本以及pipline setnx watch来实现事务解决并发问题。所有的数据都存放在内存中，通用有RDB AOF 也就是WAL+log机制，多副本一致性的复制方案，因为都是内存中的操作，速度很快，所以不需要多线程来进行提高CPU，多线程主要是用来IO写时候CPU释放来重复提高CPU利用率，同时带来了CPU切换的开销，所以redis单线程足够了，所以一般redis会部署多个独立进程，多进程单线程机制。比较复杂的有rehash机制，还有zset 我们一般用来做排行榜，或内存k,list用来存储前几千条记录的分页结构，还有bitset 有一些库实现bitmap redis实现实现去重计数。因为redis的key value都在内存中，那么其实内存限制了存放数据大小的，那么有没有只有key存放在内存中 value存放在本地磁盘索引里面的数据库呢。这样虽然延时可能差了一些，但是也能满足一些缓存要求。我接触的比较有名的就是tair 叫做cellar。其实还有很多这样的KV数据库。其实他们大体都是bitcask思想。<br> 这类数据库，Key全部存放到内存中，bitcask引擎 采取了类似0、1目录机制，对文件分片之后，分为active data file以及其他older data files,对于为此写都是相同的因为磁盘随机读写的原因，也是append机制，对于每一条记录类似于kafka记录，有自己的编码格式，比如CRC 时间戳 keysize valuesize key  value，而每次更新内存中keydir结构更新维护这每一个key对应的bitcask文件 file_id value_size value_pos记录着 bitcask哪一个文件哪一个位置，这样可以进行相应的读取，从内存中keydir读取key的value所在文件以及位置以及大小，就可以进行读取，因为append机制，所以后台会有相关的合并进程类似于hbase进行相应的合并以及生产相应的hint索引文件方便重启简历keydir。<br>后来一些更加高效的rockdb leveldb出现了<br>所以一些数据库变，在rockdb leveldb基础上使用paxso raft 算法内存中维护一致性的key信息，而进行分片副本这些机制后，最终存储相关交给节点上的rockdb leveldb来进行。统一结合一些LRU boolmfilter 在内存中常驻一些经常访问的热点数据。更有一些在这些基础上实现SQL解析层，比如apache calite SQL parser 进行SQL解析，转为相关操作方便进行相关查询，这就是MPP机制。</p>
<p>上面这些就是我基础架构大数据中间件相关工作使用的一些存储相关的认识。其实后来我发现其实这些思想其实一直没有变过，而跟重要的理解为什么这么做，必须这么做。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/08/01/incense/">分布式去中心化计算框架的思考</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-01</time><div class="content"><p>其实开发这么多年，其实基础架构那些最基本的思想从未变过，从分布式事务，分布式一致性，分布式互斥，领导者选举，并发，消息传递通讯，分片，多副本一致性，分布式容错快照，分布式控制，资源管理，一切的一切其实最底层的思想都没有变过，而现在 在中心化分布式框架都已经成熟的今天，分布式存储，分布式计算，分布式管理控制，分布式数据库 这些框架都日趋完善，5G物联网开始苗头，区块链去中心化思想开始应用，去中心化存储开始出现，如何利用未来无数的闲置的客服端，闲置的终端，闲置的移动设备，万物互联，而在此之上构建相应的去中心化分布式存储（比如现在出现的IPFS就是一个去中心化存储的框架，虽然不够成熟），构建相应的操作系统，构建相应的去中心化计算框架，用户只需要在手机上小小一点击，无数的闲置终端设备被利用起来，形成一个庞大的云，形成的一个庞大的终端，这些越来越廉价的CPU 内存 磁盘，构建起来的资源算例足够进行大规模的计算。但是跟现在的中心化分布式计算框架 spark flink storm 不同，在中心化框架下，我觉得需要解决以下几大问题。</p>
<p>  <strong>1.节点之间管理</strong>，在传统的spark 下面通过资源调度管理交给相应的RM 或者MasterNode管理。而在去中心化框架会有无数动态十万数十万，超大数据节点的加入退出的节点需要进行管理。也无法交给单一的节点维护管理这些信息，因为都可能崩溃退出，所以管理维护是一个很大的挑战。暂且不说资源隔离机制。<br>  <strong>2.节点之间资源寻址以及路由查找</strong>，传统的都是master /RM负责，而在去中心化情况下会有频繁的加入退出，怎么发现这些节点加入，怎么去中心化的维护一致性状态信息都是很复杂的工程。<br>  <strong>3.</strong>在spark中大量数据计算，是从计算往数据节点流转，本地化计算，在去中心化计算里面，怎么保证<strong>去中心化存储，并实现尽可能本地计算，任务调度分配</strong>，在中心化spark框架下任务调度分配是根据相应的并行度以及RM对NM的资源监控以及任务的分割分片从而进行相应的调度，而在<strong>去中心化中任务调度，资源分片，任务分片，尽可能保持任务粒度足够小，资源分片与任务分片足够亲和，以及资源调度足够细微化</strong>，在传统spark中资源调度算法有DRF算法<br>也就是Dominant Resource Fair算法 思想就是找到对资源比如CPU 磁盘 内存 最大最小公平实现，通过计算每个用户对每种资源的占用率选择占用率最大的值座位选项，每次从选项中选择选项最低的用户选择一个任务进行准备运行，如果有足够的资源就启动该任务进行运行，一直到不存在可用资源，大体思想是这样。但是这个在去中心化网络P2P中可能不适应，P2P中需要对带宽进行动态的管理监控，需要去中心化实现而且对P2P多租户来说这种追求过度公平可能不适合P2P相应的场景。<br><strong>4.</strong>在中心化计算框架比如spark中，<strong>分布式容错</strong>是通过RDD血统机制，一种粗粒度，flink 是lamport快照算法，较细粒度，但是去中心化节点，<strong>消息的流转，同步，以及计算任务的终止检测</strong>都是个大问题。改怎么无中心记录<strong>快照容灾恢复</strong>，怎么消息流转，怎么节点什么时候该同步快照信息记录本地快照，以及任务消息传播后怎么知道计算已经终止，终止检测问题。<br><strong>6.</strong>中心化分布式计算框架spark 数据交给blockmanger进行中心化管理，在去中心化情况，怎么<strong>资源发现，P2P路由寻找算法</strong>也是一个方面，在P2P网络波动比较大，带宽以及节点的动态加入退出，以及节点的奔溃甚至节点的人为串改拜占庭问题，这些都是去中心化分布式计算框架<br><strong>7.</strong>在中心化计算框架时候当任务停止或者NM终止时候都有RM AM维护相应的信息负责相应的重启，保持<strong>任务的完整性以及数据多副本一致性</strong>，在去中心化情况下，怎么维护一致性信息，以及当任务分片在某个节点停止了，怎么其他周边节点能够感应，进行相应的无中心化调度重启，以及当数据在某个节点丢失了，周边节点怎么感应然后进行副本容灾备份，这里面一致性算法改怎么做到。</p>
<p>正式因为去中心化情况复杂多变的网络以及庞大的数据节点所以一直没有去中心化计算框架的出现。也没有去中心化云服务的出现。这是一个很大的挑战，但是对自己来说也是一次学习成长，这也是我今后2年的学习成长目标。<br>这里我总结了一些想法以及后面一些空余时间尝试着去实现这么一个去中心化计算框架。现在有了一些思想以及设计方案，也在研究学习相关理论算法，已经大概有了一个初步的方案，将在后面去尝试实现。</p>
<ul>
<li>1.<strong>P2P去中心化拓扑结构实现、P2P资源路由算法实现</strong></li>
<li>2.<strong>一致性算法</strong></li>
<li>3.<strong>分布式快照算法</strong></li>
<li>4.<strong>分布式互斥算法时钟同步算法。</strong></li>
<li>5.<strong>分布式任务分片调度</strong></li>
<li>6.<strong>分布式去中心化数据存储</strong></li>
<li>7.<strong>分布式任务终止检测</strong></li>
<li>8.<strong>p2p资源共享使用奖励机制模式</strong></li>
</ul>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/20/感想/">感想</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-20</time><div class="content"><p>这个博客本来是为了面试时候希望面试官知道自己大概能力处于一个什么阶段的，而不希望全部都问些hashmap jvm gc 并发等 一些数据结构这些初级开发者都能知道的知识点，没想到最近还有人加我微信，来相互交流的，也许上一段时间我怕别人有些人觉得写得东西都是思想很虚的东西一样，但是最近有朋友通过这个博客搜索以前的博客资料来加我微信，突然觉得这是最好的鼓励吧，我现在工作6年多了，在工作大概4年时候那时候很羡慕佩服那些开源组件能够修改提交，真正看懂源代码的人，而不是网上那些无聊的代码剖析，为啥别人能够很清晰的掌握源代码，很容易看懂一个新的东西，后来基础架构做的多了，知识面很广， 认识了很多基础架构相关的通用本质的东西，原来万变不离其宗，这些东西在90年代 甚至80年代就已经提出了，这些东西技术从来就没有变过，而这些东西这些思想理论其实才是技术发展更高阶段更重要的东西，这是我现在的感悟，当你掌握了这些基础理论，基础也可以的时候，你会发现看源代码，去看懂一套新的分布式存储，分布式中间件，分布式计算框架 分布式管理框架，原来也不是很难，你能很容易的知道每个分布式框架是怎么组成的，就像从里到外，你知道了里，到外面，只是从组成一步步一层层分解的过程。比如你知道分布式存储的思想，你看代码，你就知道 从每个节点服务器，schema服务器调度，那个组件是维护一致性的，然后读写重要核心代码在哪里，就像一个人，你知道了他的新陈代谢，人体循环 必须有的组件以及作用，找到人体组织心 肝 脾肾肺不是很容易吗，从哪里到哪里，就是这样一个感悟，在我工作第五年时候我开始泵入了一个知识高速成长的阶段，慢慢的我写代码不担心出bug，代码开发效率很高，在美团排期两周的工作，我一天就能敲完，就是这些思想我觉得才是自己知识这个阶段最重要的东西，最近我也开始研究一些理论书籍，比如lamport计算机科学家写的相关数据理论，还有Nancy Lych 的理论书籍，虽然很复杂很繁琐，但是随着了解，发现原来那些你觉得很高深的框架很复杂的框架，那些最基本的东西，这些计算机科学家80年代 90年代就已经提出了，也许这就是道吧，当术熟练广泛时候，就是一个开始从众多术求取道的过程。但是你对术不熟练时候 你却无法了解道的神奇，这就是成长学习的过程吧，有时候觉得头几年自己小公司经历也是一种好事吧，什么都要做，知识面很广，做了很多加法，当自己去感悟这些本质时候，发现万变不离其宗，无论算法 架构 业务 原来都是一样的，这就是道吧，这也是我后面几年一直的方向，我希望沿着这条路，开始深入了解每个框架的每个细节，能够成为apache 开源组织那样的技术大牛。所以对我来说说后面一个有挑战性的工作，一个真正的大平台，才对我最重要，才会让自己走的更远。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/16/bazhanting/">拜占庭问题</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-16</time><div class="content"><p>昨天看到法国国庆7.14的飞行兵，<a href="https://autometis.oss-cn-shanghai.aliyuncs.com/2.mp4" target="_blank" rel="noopener">视频地址1</a> <a href="https://autometis.oss-cn-shanghai.aliyuncs.com/3.mp4" target="_blank" rel="noopener">视频地址2</a> 可以自由的飞行，但是很多人想如果出现电子干扰，那飞行仪器被伪造假的指令或者被干扰烧坏，怎么保证飞行人员的安全呢？这其实跟导弹原理是一样的。其实也是一致性算法，只不过问题场景不一样。<br>我们在做架构时候，多台机器分布式一致性往往通过paxso raft 来保持多机器状态配置信息一致性，比如分布式存储用于维护文件目录metadata 配置信息，但是这些算法是在中心化无拜占庭节点情况才能成立的，在很多时候，比如导弹射击目标，被电子干扰可能电子硬件设备出错故障，不仅仅是故障，有时候还会做出错误的判断，还有在两将军问题，进攻一个地方，相互之间考信使（消息传递）但是消息传递过程中如果出现地方的间谍可能会不传递消息更有可能传递假的消息，这样的情况怎么保证达成共识呢？所以一般我们分这类问题为拜占庭问题，而上面讲的我们大多数中心化网络里面的raft paxso是非拜占庭问题。但是实际上无限制情况下在异步网络中不存在达到共识的算法，但是某些时候我们可以通过一些约束，拜占庭节点（间谍伪造节点）占总结点小于三分之1时候(计算机科学家证明的上界) 是可以实现达到统一安全准确的共识。所以现在导弹发射就是多核心拜占庭错误下也能达到共识，确定实数打击目标位置，还有航天发射，还有现在的区块链去中心化网络，这些拜占庭共识算法往往用在这些完全开放的网络领域。<br>常见的拜占庭算法有 随机共识 国王 女王 还有很多，后来出现了一些改进，PBRT是比较出名的拜占庭算法，后来有些异步并行的PBRT算法，现在区块链在用它，另外随机共识 发展到后面 随机一些算法开始引入Pow 现在有些区块链币在用它。<br>PBRT算法 证明需要反证法，但是证明不重要，思路比较简单：大体是三次广播 实际上优化 可能要结合MST最小生成树还有最大独立子集还有BFS广度搜索来进行优化，另外过程可以并行异步化。源代码可以去参考一些开源的区块链框架搜索PBRT就可以看到。</p>
<p>PBFT（拜占庭容错）</p>
<p>基于拜占庭将军问题，一致性的确保主要分为这三个阶段：预准备（pre-prepare）、准备(prepare)和确认(commit)。流程如下图所示：</p>
<p>其中C为发送请求端，0123为服务端，3为宕机的服务端，具体步骤如下：</p>
<p>Request：请求端C发送请求到任意一节点，这里是0<br>Pre-Prepare：服务端0收到C的请求后进行广播，扩散至123<br>Prepare：123,收到后记录并再次广播，1-&gt;023，2-&gt;013，3因为宕机无法广播<br>Commit：0123节点在Prepare阶段，若收到超过一定数量的相同请求，则进入Commit阶段，广播Commit请求<br>5.Reply：0123节点在Commit阶段，若收到超过一定数量的相同请求，则对C进行反馈<br>记住跟paxso raft 不同他们是超半数投票，叠加<br>而PBRT需要三分之2 投票 叠加。 实际上是不是达到共识就最少需要半数投票以上呢？ 不是的其实还有一些仲裁算法可以不需要半数投票以上，这适合百万节点大量节点时候不需要百万的一半来投票，而通过仲裁机制算法结合一致性算法就可以实现共识。<br>详细可以参考这篇文章，网上也有很多模拟实现的源代码，但是实际上你最好参考一些区块链开源代码来实现，大大减少了时间复杂度。<br><a href="https://mp.weixin.qq.com/s/CE_92LZQUrH4OHNC_Doq2A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/CE_92LZQUrH4OHNC_Doq2A</a><br>现在一些PBRT 也结合Raft Paxso 来实现Raft Paxso的PBRT实现 Raft的PBRT实现，用于一些新的区块链应用。但是本质还是PBRT和Raft结合。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/12/spark/">我对spark的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-12</time><div class="content"><p>我从2014年底接触spark开始，到后面15年初创业使用spark批量计算图像特征，图像算法，PCA等，LSH hash索引后，就开始对spark产生了浓厚的兴趣，spark 为什么这么高效，为什么spark 速度要比mapreduce快那么多，spark是怎么实现的，他为什么要这么做，他从发展初到现在2.4版本似乎本质的东西一直没有变过，后来我一直使用者，也一直研究看着他的源码设计，有了一些自己的心得体会，终于明白了上面的一些问题。<br>  首先spark 是迭代计算，所以适合那些数据块的处理，不适合那种算法里面各种参数迭代计算，那个需要BSP模型，首先我们spark 分布式计算系统，对于进程来说 无非就是 进程节点状态 与 消息 本地计算的关系，因为spark设计初衷是为了适合计算大批量的数据的问题，所以对于每一个进程节点来说不可能传输数据到每一个节点上进行操作，所以是 节点是数据，而计算是边，计算在节点之间传递的模型。对于我们每一个应用来说，因为是迭代计算，所以从数据输入 到 数据输出，所以他是一个DAG有向无环图，所以spark 的job 是构造成有向无环图的，而因为计算往节点上传输，所以需要有相关的DAG调度管理器，为了多机器并行并发计算所以需要进行任务分片所以肯定也有任务调度管理器，剩下的就是怎么保证数据获取 因为大数据块存储计算 所以需要一个分布式存储引擎，为了保证容错性，需要一致性算法或者机制来保证，为了高效内存计算，所以需要内存缓存管理机制，因为分布式计算系统，多机器，每个任务需要资源不同，所以同样需要进行资源管理机制来分片资源，多台机器上也需要进行相应的资源隔离，最后需要提供相关的编程接口转化为DAG，后来引入了schema,dataframe 引入了sql 以及引擎优化，扩展了相关第三方机器学习包，以及对流数据的支持。</p>
<p>  上面说了那么多，我们大概知道 spark需要有哪些组件。这些可以通过sparkContext sparkEnv可以看得到。比如上面提到的分布式存储，用于计算中间数据存储获取的 BlockManager 为了高速内存获取计算，所以肯定要BlockManager支持memstore diskstore,为了后续扩展使用spi接口，另外前面讲需要DAG解析的 ，对于这个来说我们肯定是在客户端提交任务到多台机器之前解析的，所以我们需要DagScheduler ,对于spark 这种分布式计算架构，因为他只有在map redurce suffle时候进行通讯，他的性能决定都在于shuffle过程 所以他通过DagScheduler分解窄依赖宽依赖，而对于数据源读取统一一个RDD结构，所以数据读取时候有partioner分区，同样因为调度系统 主从架构所以需要有相关RPC通讯 链接管理发起管理命令，启动后台守护，比如用于shuffle的 MapOutputTrackerMaster shufflerFetcher connectionManager ,其他的就是一些守护线程类，因为数据存储获取我们用blockManager来管理了，但是数据一致性完整性，需要一些其他手段了，storm使用了异或特性ack 来维护一个消息的一直完整，flink使用lamport的分布式快照算法来保证一致性，而spark通过lineage 血统来实现，为了能够通过血统恢复，所以RDD会记录父亲RDD的依赖信息，同时要保证RDD只读不变特性，对于每一个spark RDD 分布式弹性数据集结构来说首先blockManager可以维护多机器上数据顺序块存储分区管理，然后当每次迭代调用时候他提供了2中，一种是transformation 一种action 前者是用于RDD之间调用，后缀RDD转为实际值时候操作，所以对于每次迭代操作，都是一个RDD转为另外一个RDD过程，另外的RDD会记录下父亲RDD信息，所以你会发现从HadoopRDD/RDD创建后面的算子 转为MappedRDD ShuffleRDD MapPartionRDD等等过程，而每一个RDD里面执行相应的算子操作，其实就是一个scala函数闭包，转换过程，RDD.iterator 而获取这时候computeOrReadCheckPoint 从缓存本地内存来试着读取数据 ，执行最基本的容器数据迭代转换操作，但是这个过程中数据每次在线程里面不断的迭代处理，其实效率有点低，所以spark做了相关编译生成代码优化，其他都是怎么获取Block块，然后怎么shuffle 的过程，因为在前面我们从Job 到 DagSchuler到Stage 到 task 其实是pipeline手段以及task任务并行化手段，这过程中内存计算迭代 shuffle （Hash-base sort-Base）手段，为了保证多Job运行资源竞争问题，所以有DRF相关资源管理算法。 其实还有很多细节方面，后面为了支持SQL 在RDD算子 上面实现了SQL schema 结构dataFrame，为了能支持spark stream流计算，把流转为一个个Dstream最后代码转为spark 的RDD任务进行执行。<br>  所以总体来说 Spark源码分析可以从：<br>   1.分布式存储BlockManager 贯穿到RDD获取存储 shuffle获取存储 spark所有方方面面里面。<br>   2.网络通讯请求Rpc akka/Netty 涉及到后台调度时后台守护线程启动相关以及内部节点相互通讯。这里面有shuffleFetcher MapOutputTracker xxxBackend shuffleBlockManager值得一看<br>   3.DAG 计算step pipeline，任务分配 task分解 所以可以看看DagScheduler 以及 TaskScheduler<br>   4.数据一致性 RDD血统实现，checkPointer相关可以结合BlockManager 看看computeOrReadCheckPoint。<br>   大体就是 网络通讯 任务pipeline并行分片调度 资源管理 文件/内存存储  shuffle机制实现等等这些,这些就是spark 源码里面比较核心的一些组件了。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/08/kafka/">我了解的kafka</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-08</time><div class="content"><p>kafka 是个很好分布式消息中间件，但是我个人喜欢把他叫做流存储，下面我从我了解的地方说一下kafka，<br>kafka 架构其实很简单，producer consumer broker 其中broker consumer使用zookeeper 维护topic路径以及消费的相关位置信息。对于用户写有同步异步写，因为中心化系统所以 一主多从，用户发送时候多个队列批量累计异步发送 ack,也有同步ack,不同的是 主 不同步消息到从，而是由从主动从主这边拉取，这样任务可以并行化，同时为了保证C一致性，只有同步好的才能被消费者消费到，但是因为主 跟 从同步的日志记录位置差一次交互，由于存在2将军问题，消息丢失超时各种可能，所以需要使用LEO HW来进行保证读取消息日志同步一直性，这之间可能会出现宕机重启问题，所以为了保证选举一直是最新消息日志的机器，会记录ISR机器，只有ISR机器才能被当选主，后面就是一些常用的套路，分片副本，负载均衡，使用topic 作为namespace隔离 然后key partioner 来做分片，多个topic不同机器master 实现了简单负载均衡，而其中用户写入的消息都有一个唯一的id ,最后通过分片写入到相应的 topic_partion的目录里面，每一个目录里面会记录2个相关文件，每个文件以 offset.index offset.data 开头，比如000000000000001.index ,为了写的高吞吐高性能，数据先写入pagecache 然后定时fsync 或者缓冲到4k 刷新到文件里面，只有刷新到文件里面的才能被consumer消费。<br>用户当读取时候 通过zk获取到offset，然后通过二分查找或者skiplist找到相应的index 文件，为了索引文件快速查找一般加载到pachecache中，同时一些最近的日志也在pagecache中，通过找到相应的index文件，读取里面记录，index为了减少大小，一般采用整数压缩，只记录一些间隔的数据，比如 000000000005 比000000000001 间隔4 会记录 （4,197） 后面表示在offset.data的197位置，而同样他不会连续的记录，比如000000000006 记录可能不会记录（5，220）这条索引，他会等待数据达到4k或者一个间隔时间 记录下次来的新纪录，索引很稀疏。同样通过二分或者skiplist找到offset.data文件，读取相应的文件。里面解析格式，里面有校验码 时间戳 id key key length value valuelength等 这样读取到数据继续，通过zerocopy方式进行发送。实现高性能。<br>因为每一条记录只会追加到文件末尾，所以文件会越来越大，最大支持4G，大体就是这样。这就是我理解的kafka,他为什么这样实现，而不是他是这么实现，这才是我们学习最重要的。其实kafka 虽然我们叫消息中间件，其实很多方面跟分布式存储很相似，其实这些思想都是不变通用的，后面我有时间更新一下spark storm flink</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/08/bigData/">HDFS的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-08</time><div class="content"><p>其实HDFS是一个很了不起的分布式存储，虽然他有很多局限性，不适合小文件，并发能力不行，但是他的设计思路确实很多产品用来进行参考改进，以及实现更强大的产品。首先说下我是从下面几方面来看待他的：</p>
<p>   首先从设计架构 它元数据schema 与 实际数据进行分离，元数据存放在NameNode 数据数据存放在DataNode,同样为了HA 可以使用zookeeper或者joneralNode机制来实现高可用性，通用套路内存 +WAL 保持元数据恢复。从数据存储角度 分块block 三副本机制，传输类似http pipeline 流传输，通用为了防止丢失引入了chunk package简单校验码技术（其实后来改进有纠删码技术Raid技术），因为数据分块存储以及追加模式，后来很多（如hbase） 结合mvcc机制定时合并,mpp计算机制在上层实现更新删除以及计算操作，但是HDFS的问题在哪里呢？<br>   1.对于HDFS小文件问题 hdfs有很多改进的patch ,引入了小文件合并的服务以及客服端。<br>   2.对于冗余方面引入了raid纠错码技术<br>   3.对于存储瓶颈引入了加速技术，从数据组织和索引技术来加速有了carbondata parquent dataleak 有cube机制，从内存来技术有了alluxio以及tachyon<br>因为很多改进方案，可以让HDFS支持小文件，支持高并发，支持更新，支持容错高可用，支持方便扩展加速所以<br>现在很多技术使用计算存储分离中很多用HDFS作为底层存储，内存加速，计算层MPP SQL层SQL化来实现相关的系统框架。最重要的是他的设计架构，很多后来改进的系统之间服务通讯也很类似。如果你是一个做基础架构的，你会怎么来设计一个基础框架呢？其实看看HDFS hbase spark flink storm你就知道的，流程逻辑贯穿框架，组件角色定义骨骼，接口应用实现功能，是不是很像一个人。 其实后面我讲的spark storm flink yarn meso等调度框架 内部看像个人，外部看像生活中的管理。正是这样在美团一年多经历让我看清了原来业务架构与基础架构其实是万变不离其宗的。业务架构中，其实业务流程 到系统流程 到基础架构过程中，只是内在外在2个分形罢了。</p>
</div><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By zhuyuping</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.0"></script><script src="/js/fancybox.js?version=1.6.0"></script><script src="/js/sidebar.js?version=1.6.0"></script><script src="/js/copy.js?version=1.6.0"></script><script src="/js/fireworks.js?version=1.6.0"></script><script src="/js/transition.js?version=1.6.0"></script><script src="/js/scroll.js?version=1.6.0"></script><script src="/js/head.js?version=1.6.0"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>