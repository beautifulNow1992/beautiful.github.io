<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="掌握java相关知识，擅长大数据，以及机器学习算法应用方面 java scala python"><meta name="keywords" content><meta name="author" content="zhuyuping"><meta name="copyright" content="zhuyuping"><title>朱遇平的github博客</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.0"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">zhuyuping</div><div class="author-info__description text-center">掌握java相关知识，擅长大数据，以及机器学习算法应用方面 java scala python</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">12</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">4</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">朱遇平的github博客</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">朱遇平的github博客</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/12/spark/">我对spark的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-12</time><div class="content"><p>我从2014年底接触spark开始，到后面15年初创业使用spark批量计算图像特征，图像算法，PCA等，LSH hash索引后，就开始对spark产生了浓厚的兴趣，spark 为什么这么高效，为什么spark 速度要比mapreduce快那么多，spark是怎么实现的，他为什么要这么做，他从发展初到现在2.4版本似乎本质的东西一直没有变过，后来我一直使用者，也一直研究看着他的源码设计，有了一些自己的心得体会，终于明白了上面的一些问题。<br>  首先spark 是迭代计算，所以适合那些数据块的处理，不适合那种算法里面各种参数迭代计算，那个需要BSP模型，首先我们spark 分布式计算系统，对于进程来说 无非就是 进程节点状态 与 消息 本地计算的关系，因为spark设计初衷是为了适合计算大批量的数据的问题，所以对于每一个进程节点来说不可能传输数据到每一个节点上进行操作，所以是 节点是数据，而计算是边，计算在节点之间传递的模型。对于我们每一个应用来说，因为是迭代计算，所以从数据输入 到 数据输出，所以他是一个DAG有向无环图，所以spark 的job 是构造成有向无环图的，而因为计算往节点上传输，所以需要有相关的DAG调度管理器，为了多机器并行并发计算所以需要进行任务分片所以肯定也有任务调度管理器，剩下的就是怎么保证数据获取 因为大数据块存储计算 所以需要一个分布式存储引擎，为了保证容错性，需要一致性算法或者机制来保证，为了高效内存计算，所以需要内存缓存管理机制，因为分布式计算系统，多机器，每个任务需要资源不同，所以同样需要进行资源管理机制来分片资源，多台机器上也需要进行相应的资源隔离，最后需要提供相关的编程接口转化为DAG，后来引入了schema,dataframe 引入了sql 以及引擎优化，扩展了相关第三方机器学习包，以及对流数据的支持。</p>
<p>  上面说了那么多，我们大概知道 spark需要有哪些组件。这些可以通过sparkContext sparkEnv可以看得到。比如上面提到的分布式存储，用于计算中间数据存储获取的 BlockManager 为了高速内存获取计算，所以肯定要BlockManager支持memstore diskstore,为了后续扩展使用spi接口，另外前面讲需要DAG解析的 ，对于这个来说我们肯定是在客户端提交任务到多台机器之前解析的，所以我们需要DagScheduler ,对于spark 这种分布式计算架构，因为他只有在map redurce suffle时候进行通讯，他的性能决定都在于shuffle过程 所以他通过DagScheduler分解窄依赖宽依赖，而对于数据源读取统一一个RDD结构，所以数据读取时候有partioner分区，同样因为调度系统 主从架构所以需要有相关RPC通讯 链接管理发起管理命令，启动后台守护，比如用于shuffle的 MapOutputTrackerMaster shufflerFetcher connectionManager ,其他的就是一些守护线程类，因为数据存储获取我们用blockManager来管理了，但是数据一致性完整性，需要一些其他手段了，storm使用了异或特性ack 来维护一个消息的一直完整，flink使用lamport的分布式快照算法来保证一致性，而spark通过lineage 血统来实现，为了能够通过血统恢复，所以RDD会记录父亲RDD的依赖信息，同时要保证RDD只读不变特性，对于每一个spark RDD 分布式弹性数据集结构来说首先blockManager可以维护多机器上数据顺序块存储分区管理，然后当每次迭代调用时候他提供了2中，一种是transformation 一种action 前者是用于RDD之间调用，后缀RDD转为实际值时候操作，所以对于每次迭代操作，都是一个RDD转为另外一个RDD过程，另外的RDD会记录下父亲RDD信息，所以你会发现从HadoopRDD/RDD创建后面的算子 转为MappedRDD ShuffleRDD MapPartionRDD等等过程，而每一个RDD里面执行相应的算子操作，其实就是一个scala函数闭包，转换过程，RDD.iterator 而获取这时候computeOrReadCheckPoint 从缓存本地内存来试着读取数据 ，执行最基本的容器数据迭代转换操作，但是这个过程中数据每次在线程里面不断的迭代处理，其实效率有点低，所以spark做了相关编译生成代码优化，其他都是怎么获取Block块，然后怎么shuffle 的过程，因为在前面我们从Job 到 DagSchuler到Stage 到 task 其实是pipeline手段以及task任务并行化手段，这过程中内存计算迭代 shuffle （Hash-base sort-Base）手段，为了保证多Job运行资源竞争问题，所以有DRF相关资源管理算法。 其实还有很多细节方面，后面为了支持SQL 在RDD算子 上面实现了SQL schema 结构dataFrame，为了能支持spark stream流计算，把流转为一个个Dstream最后代码转为spark 的RDD任务进行执行。<br>  所以总体来说 Spark源码分析可以从：<br>   1.分布式存储BlockManager 贯穿到RDD获取存储 shuffle获取存储 spark所有方方面面里面。<br>   2.网络通讯请求Rpc akka/Netty 涉及到后台调度时后台守护线程启动相关以及内部节点相互通讯。这里面有shuffleFetcher MapOutputTracker xxxBackend shuffleBlockManager值得一看<br>   3.DAG 计算step pipeline，任务分配 task分解 所以可以看看DagScheduler 以及 TaskScheduler<br>   4.数据一致性 RDD血统实现，checkPointer相关可以结合BlockManager 看看computeOrReadCheckPoint。<br>   大体就是 网络通讯 任务pipeline并行分片调度 资源管理 文件/内存存储  shuffle机制实现等等这些,这些就是spark 源码里面比较核心的一些组件了。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/08/kafka/">我了解的kafka</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-08</time><div class="content"><p>kafka 是个很好分布式消息中间件，但是我个人喜欢把他叫做流存储，下面我从我了解的地方说一下kafka，<br>kafka 架构其实很简单，producer consumer broker 其中broker consumer使用zookeeper 维护topic路径以及消费的相关位置信息。对于用户写有同步异步写，因为中心化系统所以 一主多从，用户发送时候多个队列批量累计异步发送 ack,也有同步ack,不同的是 主 不同步消息到从，而是由从主动从主这边拉取，这样任务可以并行化，同时为了保证C一致性，只有同步好的才能被消费者消费到，但是因为主 跟 从同步的日志记录位置差一次交互，由于存在2将军问题，消息丢失超时各种可能，所以需要使用LEO HW来进行保证读取消息日志同步一直性，这之间可能会出现宕机重启问题，所以为了保证选举一直是最新消息日志的机器，会记录ISR机器，只有ISR机器才能被当选主，后面就是一些常用的套路，分片副本，负载均衡，使用topic 作为namespace隔离 然后key partioner 来做分片，多个topic不同机器master 实现了简单负载均衡，而其中用户写入的消息都有一个唯一的id ,最后通过分片写入到相应的 topic_partion的目录里面，每一个目录里面会记录2个相关文件，每个文件以 offset.index offset.data 开头，比如000000000000001.index ,为了写的高吞吐高性能，数据先写入pagecache 然后定时fsync 或者缓冲到4k 刷新到文件里面，只有刷新到文件里面的才能被consumer消费。<br>用户当读取时候 通过zk获取到offset，然后通过二分查找或者skiplist找到相应的index 文件，为了索引文件快速查找一般加载到pachecache中，同时一些最近的日志也在pagecache中，通过找到相应的index文件，读取里面记录，index为了减少大小，一般采用整数压缩，只记录一些间隔的数据，比如 000000000005 比000000000001 间隔4 会记录 （4,197） 后面表示在offset.data的197位置，而同样他不会连续的记录，比如000000000006 记录可能不会记录（5，220）这条索引，他会等待数据达到4k或者一个间隔时间 记录下次来的新纪录，索引很稀疏。同样通过二分或者skiplist找到offset.data文件，读取相应的文件。里面解析格式，里面有校验码 时间戳 id key key length value valuelength等 这样读取到数据继续，通过zerocopy方式进行发送。实现高性能。<br>因为每一条记录只会追加到文件末尾，所以文件会越来越大，最大支持4G，大体就是这样。这就是我理解的kafka,他为什么这样实现，而不是他是这么实现，这才是我们学习最重要的。其实kafka 虽然我们叫消息中间件，其实很多方面跟分布式存储很相似，其实这些思想都是不变通用的，后面我有时间更新一下spark storm flink</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/08/bigData/">HDFS的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-08</time><div class="content"><p>其实HDFS是一个很了不起的分布式存储，虽然他有很多局限性，不适合小文件，并发能力不行，但是他的设计思路确实很多产品用来进行参考改进，以及实现更强大的产品。首先说下我是从下面几方面来看待他的：</p>
<p>   首先从设计架构 它元数据schema 与 实际数据进行分离，元数据存放在NameNode 数据数据存放在DataNode,同样为了HA 可以使用zookeeper或者joneralNode机制来实现高可用性，通用套路内存 +WAL 保持元数据恢复。从数据存储角度 分块block 三副本机制，传输类似http pipeline 流传输，通用为了防止丢失引入了chunk package简单校验码技术（其实后来改进有纠删码技术Raid技术），因为数据分块存储以及追加模式，后来很多（如hbase） 结合mvcc机制定时合并,mpp计算机制在上层实现更新删除以及计算操作，但是HDFS的问题在哪里呢？<br>   1.对于HDFS小文件问题 hdfs有很多改进的patch ,引入了小文件合并的服务以及客服端。<br>   2.对于冗余方面引入了raid纠错码技术<br>   3.对于存储瓶颈引入了加速技术，从数据组织和索引技术来加速有了carbondata parquent dataleak 有cube机制，从内存来技术有了alluxio以及tachyon<br>因为很多改进方案，可以让HDFS支持小文件，支持高并发，支持更新，支持容错高可用，支持方便扩展加速所以<br>现在很多技术使用计算存储分离中很多用HDFS作为底层存储，内存加速，计算层MPP SQL层SQL化来实现相关的系统框架。最重要的是他的设计架构，很多后来改进的系统之间服务通讯也很类似。如果你是一个做基础架构的，你会怎么来设计一个基础框架呢？其实看看HDFS hbase spark flink storm你就知道的，流程逻辑贯穿框架，组件角色定义骨骼，接口应用实现功能，是不是很像一个人。 其实后面我讲的spark storm flink yarn meso等调度框架 内部看像个人，外部看像生活中的管理。正是这样在美团一年多经历让我看清了原来业务架构与基础架构其实是万变不离其宗的。业务架构中，其实业务流程 到系统流程 到基础架构过程中，只是内在外在2个分形罢了。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/06/17/realtimeCompute/">我了解的大数据平台架构发展</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-06-17</time><div class="content"><p>2015年初，我进行创业，那时候storm实时计算开始流行。于是storm 实时计算 以及 spark 离线计算开始分别解决实时场景增量计算以及离线统计分析用于计算机视觉实现AR 图像识别方面。后来创业失败来到喜马拉雅平台架构，开发了分布式日志监控系统，分布式大数据平台，才明白原来这就是lambda架构，<br>将实时计算 批处理 以及 离线计算 以及服务层 分为多层，实时计算/微批处理用kafka 接入进行统计<br>计算写入，离线计算用kafka通过flume同步hdfs进行计算写入hbase es等其他。用反压以及markertime保证时效性，实时统计变量以及离线统计变量合并写入结果，hdfs、kafka进行presto BI相关分析，这一套架构很多年了，但是我认为的他有很多缺点，一方面是数据大量冗余存储各类es hbase redis hdfs kafka中以及数据丢失不一致CAP问题，还有一个缺点就是逻辑多套可能相同的，但是要部署几套逻辑，写几套代码。后来为了防止这个问题，我实现了一个sdk封装类似自己演示项目，通过stepbuilder模式 DAG封装 自动生成spark structstream代码 以及 sparkcore代码 进行了代码层的统一逻辑。不需要在流处理以及离线处理各写一套逻辑了。后来我发现了存在一些问题就是在做风控过程中，变量计算 spark流统计后计算的变量缓存在pika/redis里面，但是histroy data怎么合并,于是在想是否除了lamabda架构之外还有什么相关架构。后来了解到有kappa架构，他将就的是统一数据源，对于流处理以及离线处理统一从数据源中进行<br>生成相应的job，流处理写入中间存储缓存里面比如redis hbase中，离线存储写入hdfs 等永久存储里面，然后生成一套维护代码，定时的吧中间存储 合并到 离线存储中。这一套逻辑就是我上面演示项目做得，我试着封装统一API API生成相应的job代码，流计算代码，离线计算代码，然后中间存储与离线存储合并。用于股票分析。实时记录与历史记录，实时记录走flink、spark stream ,离线记录 spark ,代码通过自动生成统一逻辑层,元表管理层，然后在这SDK之上维护一个UI层以及整合机器学习层以及SQL层这是我那时候的打算。后来发现这种架构也有一些缺点。kappa缺点在于很难统一数据源，特别是流处理与离线处理，job的管理调度，元表管理，以及历史与中间缓存的合并。但是现今很多企业大数据平台架构大体都是这2种。<br>   前一段时间我在想难道没有一些新型的架构吗？后来去查询是否有解决统一数据源的分布式流存储系统，于是了解到了prevage ，新型的实时流存储系统，相比kafka来说，他分层存储，统一了上面说的流实时中间缓存存储问题，以及永久存储问题，数据无界以及永久存储问题，当然还有很多新特性，相当于解决kappa 的缺点，数据源统一、读写存储统一，大大减少了冗余，就跟flink 把所有的当做实时计算，微批处理只当做其中特例一样，于是一种flink/spark + prevage<br>架构开始出现。于是近期我在想自己空余学习项目是否需要进行改进扩展，我的打算是在prevage以及 flink之间实现一层包装层，从flink 每个算子以及机器学习各类操作，转为flink+prevage的操作代码，对用户透明，提供sql接口，这样大数据相当于实现一个类似的数据库，无论是实时流处理 微批处理 或者 离线处理，相当于数据库的表的操作处理。平台既是数据库，屏蔽了所有大数据离线实时各类计算不一致问题。对外提供查询 机器学习训练预测功能。</p>
<p>未来5G开始流行，物联网设备，大量数据产生，实时性流计算，实时预警感应越发重要，大数据方面个人比较看好分布式流存储，以及 去中心化分布式计算 ，去中心化流式云存储问题。一直在研究这个方面。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/06/05/NAS研究/">NAS 我眼中的AutoML</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-06-05</time><div class="content"><p>我在空余时间做的一个展示项目其实核心并没有完成，特别是对于一个DAG有向无环图来说怎么让机器自动找到一个最优的DAG才是最核心的关键内容，最近google新出的efficentNet模型就是基于NAS的automl得到的模型作为baseline然后 动态优化相关的深度精度宽度来实现更高的基准，所以最近这段时间个人一直在看NAS，一方面分布式计算是否可以用NAS来优化，另一方面automl 实现NAS 详细大概是怎么做的呢？</p>
<p>说到automl 我们要了解做算法 做大数据大体流程，<br>数据接入读取，数据清洗转换，数据特征提取，特征选择，数据模型训练可视化，超参数优化 这几个步骤，更加进一步就是automl 也就是超参数优化自动化，模型生成自动化，进而拓扑图生成自动化， 但是自动化要解决的问题就是怎么样生成最优的一个拓扑结构呢？</p>
<p>我了解最近的研究大体分为下面几个方面：<br>首先目标是 寻找最优的拓扑结构，寻找最优的超参数<br>对于每一个节点来说节点之间怎么连接组合，对于最优结构来说可以从图论考虑，寻找最优子图，可以从系列考虑，使用RNN LSTM Transformer来一层层生成,对于每一种结构的超参数就是搜索优化问题，这样可以把结构以及参数作为网络的输出，进行生成。</p>
<p>强化学习：</p>
<p>我以前在进行NAS尝试，可以效果不理想，还有一些问题没解决，还在研究，那时候使用的是强化学习，通过训练生成相应的子网络，子网络使用校验数据进行预测的精度MSE等加权作为Reward进行反馈，从而进一步调整，子网络，而每一步节点或者相应的结构块block cell使用RNN生成，超参数另外作为一个输出，输出多个参数，但是当时块与各个层之间连接太复杂了没有解决这类问题。</p>
<p>同样只要了解到NAS 目标是寻找一个最优子结构，最优超参数问题，所以实际上是一个最优化问题，所以可以用贝叶斯 遗传算法 梯度算法 线性规划等等。</p>
<p>最近在研究分布式算法，一直想NAS 是否可以用来解决分布式算法的某些问题这应该是一个有趣的问题？而且自己还没想通某些网络结构比较复杂，怎么整合到NAS常用的卷积网络，全连接，激活函数，链接结构，RNN结构，LSTM结构，这几类比较简单整合，但是transformer以及GAN 以及 VAE 这类改怎么整合呢？</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/20/logicTime/">谈谈我对逻辑时间的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-20</time><div class="content"><p>在分布式一致性算法中接触到了逻辑时间概念，逻辑时钟概念被用于分布式快照算法以及分布式一致性算法，比如上面的lamport快照算法以及亚马逊的dynamic分布式存储。比如vector clock<br>在分布式系统中，我们无法保证全局的一个统一时钟，虽然有NTP 相关保证ms内的时间同步，但是在高并发进程消息通讯中 由于消息丢失，超时各类问题，以及拜占庭问题，消息传输会导致乱系 丢失 篡改 重复传送，比如在storm中 算子代表节点，边代表处理的数据消息，消息进程先后顺序无法用全局时间戳来进行表达，所以出现了逻辑时钟的概念，lamport 1978年提出。</p>
<p>  对于多个进程之间，如果2个进程之间，A发送消息到B ，那么必然有A B之间的依赖，也就是说 A的时间必然小于B的时间。存在因果依赖关系，逻辑时间有三种方式，标量时钟 向量时钟 矩阵时钟，但是这不重要，因为进程之间，无非是内部事件操作，发送，接受，所以我们要保证的是当执行操作时候本地时钟的变化，以及发送时候 接受时候 时钟改怎么记录更新全局时钟。所以有2个规则：<br>  A规则：<br>    当进程执行本地操作事件，发送事件，接受事件时候本地时钟该如何变化。<br>  B规则：<br>    当进程发送接受时候 本地时钟以及全局时钟如何更新。</p>
<p><strong>标量时钟</strong>：就是通过一个标量记录每一个进程来记录全局时钟以及本地时钟。<br> A。 当用户执行发送接受以及本地事件之前进行本地时钟更新， 本地时钟进行+N操作<br> N一般取1。<br>B。 当用户发送事件时候携带 本地时钟c 到消息中，当另外进程收到消息时候，获取消息中携带的本地时钟c 进行更新<br>改进程本地时钟更新为Max(c，本地时钟)，因为是接受事件，所以符合规则A定义，所以再执行规则A。</p>
<p><strong>向量时钟：</strong><br>向量时钟就是通过一个向量记录1个进程看到的所有其他进程包括自己的时钟状态。<br>比如V（i） 表示 低I个进程的时钟状态。假如进程数为N 那么，每一个进程通过消息传递因果关系得到的信息，记录着自己所知道的各个进程的本地时钟。</p>
<p>A：<br>  当进程I执行相关事件时候<br>  V（i）= V（i）+N<br>B:当发送事件时候 携带V发送到消息M中当进程收到消息时候，获取消息M中的V，并更新V所有的值为本地事件和V的最大值。因为是接受事件，所以符合规则A定义，所以再执行规则A。</p>
<p>矩阵时钟也类似。</p>
<p>大体是这样，一般很多文章都会统一说vector clock 大体一个意思.</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/17/distributeStore/">分布式的一些想法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-17</time><div class="content"><p>接触过很多分布式系统，以及内部各种设计，想分布式管理系统，分布式计算系统，分布式存储/数据库 ，大体市场上的都可以分到这几类，负载均衡有一套体系，自动扩容有一套体系，限流，分配 副本 一致性都有专门的相关算法知识，头几年我那时候很困惑，这些系统到底为啥要这样做，架构架构什么才是架构的目的，用这么多算法理论，为什么要这么做，最终是为了什么？大体我总结到的原因是这么几类：</p>
<p><strong>外层来看：</strong></p>
<ul>
<li>1。从业务流程抽象到系统流程并架构化。主要用来解决通用业务问题以及后续的扩展问题。</li>
<li>2。开发职能分离单一化，通用化，易管理扩展以便能够系统之间开发维护故障影响依赖解耦。主要是外层的一些设计思想SOA 微服务 servermesh网格</li>
</ul>
<p>这一类是大体是 从产品业务理解，项目管理 ，开发效率，项目维护迭代等方面做出的一些改进的各类思想。就像人的外貌 体格 身高。</p>
<p><strong>内层来看：</strong></p>
<ol>
<li>性能以及瓶颈能否支持业务以及后续发展需要。所以各种无锁 pageCache 内存 异步 新的结构等等机制出来解决这类问题</li>
<li>可伸缩性，就是时间，效率，性能是否支持横向纵向的扩展，所以各类分片 负载均衡 调度管理算法出来解决这类问题。</li>
<li>灵活性 就是产品业务方面能否适应后续的改动变化，拆分 组合 所以各类设计模式以及思想出来了解决这类问题。</li>
<li>可扩展性，自动化 就是一些通用的东西，能否适应后续业务产品需要，方便开发扩展适配，所以各类产品业务都有一个最终提炼的思想核心，每个人根据他对产品业务了解来设计。</li>
<li>后面的就是CAP BASE问题 怎么保证系统可用可靠性 ，故障转移，多活 ，自动扩容，备份容灾恢复，负债均衡，点对点，还是主从，还是读写分离。怎么保证数据一致，在这中间怎么取舍，各种异常情况可能性都需要考虑。</li>
<li>可维护编排部署管理性，主要从易用，测试，管理 维护角度来考虑</li>
<li><p>最后就是一些通用的需要的UI 监控管理层。这个可有可没有</p>
<p>这一类就像是 人的本质。</p>
<p>最近在研究一些东西，一方面像自己实现一个分布式计算的应该怎么做？自己实现一个分布式存储该怎么做？我要考虑哪些方面，要怎么改进，要怎么做，另外一方面自己 不用一些库能否写出来相关算法。这是我后面要一直成长研究的方向。还有很多陌生的知识需要去工作生活空余了解研究。 就像我用了很多图的思想，但是图从最开始的基本图的bfs dfs 一些基本算法，到图中基于一些特性的图搜索查找，挖掘的图算法，到最新的图嵌入，到图深度学习，到其中一个分支 知识图谱。可谓是博大精深。但是每天进步一点点，最后总会厚积薄发!</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/15/distributeChandy-Lamport/">我理解的分布式快照算法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-15</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/chandy-lamport快照算法/">chandy-lamport快照算法</a></span><div class="content"><p> 对于分布式计算系统，分布式系统数据流动变化时候，往往需要记录一个全局状态也就是快照，以便系统崩溃时候能够通过快照得到每个节点本地快照并恢复。想象一个场景，城市交通四通八达，有很多下水道，蚂蚁是怎么找到并记录道路状况的呢？每一条道路派一支蚂蚁去探路，那如果我们要拍一张照片，全局的快照，但是我们一次性拍不下全部，那我们怎么做呢？我们就每一条分支路径拍一张快照，那么所有的本地快照就可以组成一张全局快照，那每一张本地快照改记录什么呢？对于分布式系统，有是数据从边流向节点，或者操作从边流向节点，进程之间也是一样，那么怎样保证记录快照呢？ 我们这样想，对于当前节点状态我们肯定是要记录的，对于当前节点输入的也是要记录的，输出呢？不需要，因为我们可以通过节点状态以及输入可以得到输出，所以只需要记录状态以及输入就行，但是输入什么时候该记录呢，我们拍照肯定是拍照点之前快到拍照这个点的数据，就像漫画里面一个个慢动作，最后我们眼中看到的是慢动作组成的动作，但是我们快照就是一个个慢动作的帧，是我们最后眼镜拍照那个状态之前的帧<br>    下面说一下<br>     <strong>Chandy Lamport 算法<br>这个算法把P进程-&gt;Q进程数据交互分为四个部分，其中因为每个节点时间上的不一致，所以采用了一个marker 来记录什么时候该拍每一个节点的快照，所以分为4个过程。也就是2个初始态 2个中间态，</strong></p>
<p>如果当进程拥有marker状态记为s1,进程未拥有token状态记为s0。<br>1.P进程开始发送marker 到Q进程 此时 P Q  应该为[ p：s1 q：s0] 这时候相当于漫画我们开始要画第一帧，此时人物 环境状态 一张静态图。<br>2.P已经发送了marker了在传输的channel边上 即将到Q进程<br>这时候 P Q 应该为 [ p：s0 q：s0 ] 哈哈，此时想象变化的是marker的channel吧，这时候相当于漫画我们开始画第二帧 第二帧 第几帧的中间过程了。多张动画帧<br>3.进程Q收到marker marker已经到了Q ，这时候相当于我们动画里面 一个片段。 [ p：s0 q：s1 ]<br>如果有双向的场景，我们倒退场景，顺序 逆序<br>4.这时候进程Q 发送marker到P<br>同样有[ p：s0 q：s0 ]<br>5.这时候进程P 收到了来自Q的marker 同样有 [ p：s1 q：s0 ]<br>那么他怎么快照呢？<br><strong>实际过程很像： 漫画从手绘到动画的过程</strong>。</p>
<p><strong>我们要快照时候相当于画动画初始帧，但是这个帧有很多后续变化，所以<br>第一步 先记录初始帧状态，然后当记录成功后发送一个marker拍快照的标记给后续所有帧，<br>这时候遇到后续帧会有如下情况，<br>如果后续动画帧 我们没有记录下来，那么我们要记录当前这帧，同时，我们知道这之前的所有帧已经记录好了，如果实际上发现这一帧已经记录了，说明后续动画有从当前帧开始变化了，下一个片段了，这时候是不是要记录从从上一个片段记录帧完成后 到 这个片段开始记录帧marker之前所有的帧，也就是channel信息</strong></p>
<p>下面是原论文信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//（1）（2）</span><br><span class="line">begin </span><br><span class="line">         p  records its state;</span><br><span class="line">end    </span><br><span class="line">then </span><br><span class="line">        p sends one marker along c after p records its state and before p sends further messages along c.     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//（3）（4） </span><br><span class="line">if q has not recorded its state then </span><br><span class="line">      begin </span><br><span class="line">                 q records its state; </span><br><span class="line">                 q records the state c as the empty sequence </span><br><span class="line">      end </span><br><span class="line">else </span><br><span class="line">       q records the state of c as the sequence of messages received along c after q’s state was recorded and before q received the marker along c.</span><br></pre></td></tr></table></figure>
<p>晦涩难懂但是我是这样理解的，像漫画一样。当其中的某一个片段出问题了，需要回复时候，我们可以通过快照，找到之前上面marker 拿到channel信息 重新计算，这样就可以保证分布式流动状态一致了。<br>其实所有的这些是为了保证2个原则：<br><strong>在本地快照之前：</strong><br>  <strong>进程发送的任何消息一定要记录到全局快照里面</strong></p>
<p><strong>本地快照之后：</strong><br>  <strong>进程发送的任何消息一定不能记录到全局快照里面</strong></p>
<p><strong>所以 从发送的角度考虑<br>从接受的角度考虑，从节点考虑，从channel 考虑。来保证上面2个原则就是其精髓。</strong></p>
<p>所以你会看到 P ===&gt; Q<br>marker是为了区分发送前的消息与发送后的消息，1，2过程为了保证  <strong>在本地快照之前：</strong> 3,4过程是为了 保证 <strong>本地快照之后：</strong> </p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/09/spark理解/">我理解的分布式计算系统</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-09</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/spark-storm-flink/">spark storm flink</a></span><div class="content"><p>接触spark从2015年出现火爆开始到至今。其中接触并使用过storm 使用过现今比较流行的flink，还记得2014年底，我在一家医疗分析的创业公司时候，工作需要，研究并接触到了并发批处理，那时候mapreduce开始出现，接触到了disrupter并发框架，接触到了netty网络通讯一些东西，那时候想是不是通过netty网络通讯保持多台机器并发并行的执行并发框架是不是可以实现分布式计算呢，那是我第一个初步的理解，后来从写代码到熟悉api 到了解一些细节。并心中对spark storm 到 flink 一些归纳总结思考后，我提炼了一些自己的认识。下面说说我眼中的分布式计算。<br>   在传统高并发 有多线程  多进程，比如多线程共享内存方式，多进程消息传递方式，到充分利用特性，多进程并行，多线程消息传递机制，进而到分布式多机器多进程多线程多协程，所以出现了各类分布式计算框架，我总结的大体分类几类：第一类 是基于mapreduce 算子拓扑图的 比如mapreduce storm flink spark 第二类 基于通讯机制 消息之间或实现共享内存共享线程等 ，通讯调度计算的 这一类有ignite xgboost ps-servier 参数服务器这些 第三类基于 图 节点传播pregel模式的 ，大体从数据来说 可以从图的 顶点和边来考虑。一类数据比较小 所以数据是边 顶点是算子 比如storm flink,一类数据较大 所以数据是顶点，算子是边。<br>   大体来说 现在第一类算子拓扑图的分布式计算大体模式是通过抽象出操作作为一些算子，算子之上封装成一些table schema dataset sql一些抽象，对于底层算子之间 只有shuffle 才会进行通讯其他之间不通讯，而算子之间通过有向无环图DAG进行管理划分，对于每一个Job 大体都是 分布式存储抽象层<br>任务调度分配层 网络通讯层 资源管理隔离层，动态跟踪监控层，然后为了保证一致性 通过分布式快照算法来实现流程数据一致性流程完整性。这个设计思想 在其他分布式存储 分布式中间件中很相似也有异曲同工之妙。最近看到flink 开始采用akka 消息通讯机制来替代原来的共享内存锁机制，如果后面flinkml需要支持深度学习的化，也许flink akka实现会向第一类allreduce机制来兼容。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/08/cappaxso/">谈谈自己对分布式一致性算法的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-08</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/paxso-raft-gossip-qurorum/">paxso raft gossip qurorum</a></span><div class="content"><p>说起分布式一致性算法，就要讲到CAP原则，我们在多机器之间、多副本之间如果不需要强一致性，直接读写 主从（半）异步同步就行，但是总是有一些场景，需要保证每次获取到结果都是一致性的，比如资源注册管理一些metadata schema信息，一致性状态，各个副本或者节点之间操作严重依赖彼此数据一致，这时候需要保证一致性或者最终一致性。总体来说现有一些zookeeper consul etcd atom一些开源框架实现了这些一致性算法。<br>这类算法有个基本思想就是（NWR除外）对于每台机器相同的初始状态，或者某个时间相同的初始状态，如果经过的状态系列变更相同最后的结果一定是相同的，同样对于相同的变更系列分为2种一种是基于过程的变更系列 这叫做复制状态机，比如paxso raft 记录变更的前后日志信息，还有一种是基于结果的变更系列 这叫做primary-backUp。比如zookeeper ZAB基于数据结果的变更系列。在了解分布式一致性算法前，请先知道一个问题什么是2PC，网上文字很多这里就不说了，其实网上很多paxso 算法太多抽象难懂，我也看过很多，但是太过晦涩难懂，下面就用我理解的方式来说吧，我以前也没怎么搞懂，后来看了一下mutipaxso一些框架实现的源码，渐渐的懂了。首先我们知道 2个角色，一个提案者 一个议会者，<br>他是怎么做的呢？ 让我们想象一个场景，读书时候，学校搞庆祝活动，<br>我们一大群人去操场抢位置，每个人都想抢到一个靠前的座位，比如 1–20 20-30 30-40 ，但是实际上很多空位，有可能一部分同学去厕所了，去组织表演了，这时候来了一个同学（请求）我先看中1号座位，所以他会做这些事情， 我想周边的在的人询问 这个1号座位 有人占了吗？但是每个人知道的可能不一样，因为也许别人没有关注这些，根本不知道，但是对于每个人来说他总不知道，或知道<br>我知道哪些之前位置座位有人占了，我知道几号座位刚刚谁占了，但是我们不可能记住每一个座位，但是我们知道 1 2 3 4 5 6 7 这数字顺序是不变的，如果我们能按照这个顺序来占的话，一步步确认保证，是不是从1 、、、到N被占信息如果都确定了，只需要知道 某个N就行了，<br>好，下面我们做2次询问来看看能不能占当前这个座位N 。<br>   第一阶段：<br>   A.我们发起第一次询问，问旁边的人，座位N 有人占了吗？ 所有人收到消息的根据自己记得信息答复，比如他记得N之前的之前的座位都被占了但是反正当前N没被占，那么他就答复 可以占ack，否则不答复，同时这个答复的人心里打了小九九，知道了有人对N座位有人想要占了，<br>   这就对应paxos的</p>
<p>   <strong>Prepare阶段：<br>Proposer选择一个提案编号n并将prepare请求发送给 Acceptor。<br>Acceptor收到prepare消息后，如果提案的编号大于它已经回复的所有prepare消息，则Acceptor将自己上次接受的提案回复给Proposer，并承诺不再回复小于n的提案。</strong><br>其中座位号是编号N，要记录的信息是<br>（minProposerId,acceptId,acceptValue）是其他同学心里记得占了的信息 只需要记得哪个之前的座位占了就行，这样同学来问，我就知道该回还是不回了。<br> 第二阶段：<br> B.当超过半数的同学都回我，告诉我这个位置现在还没有占，应该可以占。这时候我肯定去确认并试图占下来，我就告诉其他同学，我要宣告我要占这个座位了，如果这个我们去占的时候，还没人占，好，那我们占下来，其他同学也知道了，心里记下了，座位N已经做了谁啊，N之前的都已经确定了，但是询问发现假如这期间另外一同学已经把位置占了，这时候我们知道泡汤了，我们只能默默的让给他，自己知道N已经被谁做了，但是我们可以去抢N+1的座位吧，重新进行询问。</p>
<p> 这就是paxso 第二个过程</p>
<p> <strong>Accept阶段：<br>当一个Proposer收到了多数Acceptor对prepare的回复后，就进入批准阶段。它要向回复prepare请求的Acceptor发送accept请求，包括编号n和根据prepare阶段决定的value（如果根据prepare没有已经接受的value，那么它可以自由决定value）。<br>在不违背自己向其他Proposer的承诺的前提下，Acceptor收到accept请求后即接受这个请求。</strong></p>
<p>总体来看就是通过座位 1 &lt; 2 &lt; 3 &lt;4 &lt;n 这种有顺序的简单数字顺序，我们来放每次接收到的变更命令请求 一个坑一个坑的往里面放，这有什么好处呢，因为这样有顺序，一旦多数投票节点已经确定知道了某个K 坑已经被占了，代表这这些人知道原来之间1 …到k-1是被占了的，这样不管怎样，也就是说2次询问，不管中间那些节点丢失了链接失联了，他只要发起1….k-1坑的询问，<br>通过多数投票的这些节点知道2次交互，<br>每次都肯定返回同样的结果,而且也能得到原来我不知道的那些坑已经有人占了啊的信息，这相当于查询，这样那些不知道的失联节点可以通过这样询问，同步更新自己知道的信息，最终保证一致性。</p>
<p>这样应该清晰了吧，这是我的理解，其实这basic paxso 有2个问题，1.1个是活锁问题，第二个是2次询问交互时间问题，所以在mutipaxso 问题上就改进了，活锁是因为多个同学可能同时都要抢座位并发问题，导致有些同学一直没抢到一直问下一个下一个座位，问的都快哭了，所以选出一个班主任leader，通过班主任来统一进行询问安排座位，2.2次交互时间问题，因为统一了班主任来问了，所以班主任在问的时候不要每次都问，只需要说我要询问安排座位了，然后就问这个作为能不能做，那个能不能做就行。</p>
<p>今天回家有点晚了 ，先说到这里吧。<br>后面有时间补充raft zab 一些改进的思想，其实跟mutipaxso很像。就像paxso 里面的同学记得n之前的座位被占了信息，在raft里面就是日志以及commitId,下次有时间在补充。</p>
<p>今天接着讲一下 我对Raft的理解，2013 年raft算法开始出现，其实Raft 有点类似上面的paxos的mutiPaxso改进，mutiPaxso没有要求保证同一个时间同一个任期只有一个leader,但是Raft算法保证了，那应该怎么保证呢？ 我们回想一下paxso上面讲的那个N 以及 N-1 之前的座位确认，我们就很快想到，我们把任期也做成一个N N-1之前的座位号确认就行，在Raft 他叫做term ，同时以前的同学占座的那个N 是我们最终要每个同学都一致认同的已知信息，这个N 在Raft 里面就是他的日志的commitIndex ,所以思想很相似吧，有时候你把raft当做paxso的改进也未尝不可。对于Raft来说保证每一次只有有一个leader 所以他就有一个选举阶段。选举跟上面paxso一样，肯定要那个term最大，知道的信息最多（commitIndex最新）的人才能当选班主任啊，<br>啊，同样成为leader后 班主任就要同步我知道的信息给现场同学或者一些刚到的同学，所以有一个消息同步阶段，这里想象班主任跟同学同步信息，怎么做，肯定是说我这边到N的座位都是已经确定的，同学心里想我只记得到K的座位都是确定了的，那么我把班主任知道的K-N之间的信息记下来，这样我也知道到N的所有座位情况了，如果班主任跟我知道座位做的人不一样，那肯定是我错了，我就删除更新记下班主任知道的情况，其他的各类情况反正根据情况考虑就行了，这就是第二个阶段 ，假如下午时候班主任突然生病住院了，是不是会有代课老师来负责安排这些工作，这时候 就是第三阶段重新选举并恢复阶段，代课老师成了代理班主任，这时候肯定给所有同学做一个宣告，广播，告诉同学们，最近这段时间我代理班主任职务来负责，这时候同学都记得了，term+1了，最新的记忆里面都知道了最新的班主任是谁，但是实际上raft 跟 现实有点不一样，假如班主任又突然好了回来了，这时候本应该是他回来当班主任，但是在raft里面就不一定了，因为当前的是term+1比班主任term要大，班主任成不了leader了，只能等代理班主任当完代理完走，下次再去选举了，所以如果按照这个场景来分析，其实可以实现leader节点群，follower节点群，leader节点就是班主任有其他备选的一些，也许做做投票议会，优先级权利比follower节点更高，kafka里面这有点类似ISR，只有ISR（班主任病了时候有资格代理班主任的）才能当选班主任。就讲这么多吧。ZAB 跟Raft很类似。大家可以相同的理解就行。但是里面同步消息有点不太一样。</p>
</div><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By zhuyuping</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.0"></script><script src="/js/fancybox.js?version=1.6.0"></script><script src="/js/sidebar.js?version=1.6.0"></script><script src="/js/copy.js?version=1.6.0"></script><script src="/js/fireworks.js?version=1.6.0"></script><script src="/js/transition.js?version=1.6.0"></script><script src="/js/scroll.js?version=1.6.0"></script><script src="/js/head.js?version=1.6.0"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>