<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="掌握java相关知识，擅长大数据，以及机器学习算法应用方面 java scala python"><meta name="keywords" content><meta name="author" content="zhuyuping"><meta name="copyright" content="zhuyuping"><title>朱遇平的github博客</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.0"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">zhuyuping</div><div class="author-info__description text-center">掌握java相关知识，擅长大数据，以及机器学习算法应用方面 java scala python</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">5</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">4</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">朱遇平的github博客</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">朱遇平的github博客</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2019/02/20/automl/">个人感想以及介绍</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-02-20</time><div class="content"><h3 id="个人思考"><a href="#个人思考" class="headerlink" title="个人思考"></a><strong>个人思考</strong></h3><p>   知识像孤岛，你了解熟悉的知识越广，海岛越大，岛屿边缘海岸线就越长，接触到的未知就越多,感叹自己的无知，我认识了架构业务算法的异曲同工之妙，惊叹设计思想的精妙绝伦，充满着对每一个细节的求知欲，充满着对未知新事物的好奇心，就像发现新大陆的孩子，只想探索每一片角落。仿佛精力无限，只想探索未知给自己一个答案。</p>
<h2 id="求职意向："><a href="#求职意向：" class="headerlink" title="求职意向："></a><strong>求职意向：</strong></h2><p>   希望公司稳定有挑战有发展的大平台，头几年人生道路选择上历经坎坷，小公司要倒闭了，非互联网公司互联网部门快撤销了，创业失败了，从18线互联网一步步爬上来付出了无数的勤奋努力，18年选择来美团没有选对自己的岗位方向，也是人生道路的经验吧，所以下一份工作希望能够对自己有挑战性，方向合适的，能够有机会展现自己的能力的大平台，学习成长自己，因为以前人生道路不顺，简历职业生涯跳槽频繁了，所以下份工作不可能在换了，不然除非不在互联网这个行业了，我对自己的规划也是：技术上.非技术上 软硬实力成长，1.技术上：希望挑战性 复杂性工作，自己想深入系统核心，尝试挑战优化极致，追求完美，自己在空余时间也计划去深入研究分布式存储计算，中间件，分布式计算 其他平台领域，进行源码的庖丁解牛，深入底层，2年内自己有造轮子的能力，2.非技术上提高自己看待事物的方法，沟通理解业务能力，总结一套自己的思想方式模式，对事物能够一个全局观，以及某方面有深刻的认识。  </p>
<h2 id="个人介绍："><a href="#个人介绍：" class="headerlink" title="个人介绍："></a><strong>个人介绍：</strong></h2><p>以前在喜马拉雅平台架构，做了分布式日志监控系统（类似我现在所在美团的Cat+logCentor+天网+XT+数据仓库），分布式风控系统(第三代规则引擎+模型算法)，分布式大数据平台，数据仓库相关核心开发，后来来到美团，负责风控系统rcData数据平台，以及rcfraud审批平台，以前喜马拉雅风控是第三代第四代，这里是第一代。在这期间完成了审批流程抽象抽取，生活费异步审批的封装，人行报告的解析落地。批量大数据处理贷后数据。个人java基础知识扎实，高并发，因为架构qps高，所以对于高并发场景，日常中幂等，降级 熔断 自动扩容，负载均衡，服务化，限流，缓存，分库分区分表，性能调优，大数据，分布式中间件kafaka ,相关newSQL nosql,监控报警，服务编排等手段来解决这方面的问题，术业有专攻，个人比较擅长大数据以及大数据相关体系的技能。懂机器学习算法原理，能够熟练运用使用机器学习算法,深度学习也能运用,在喜马拉雅以及美团以及以前工作有过这方面的开发经验。语言方面：java scala 比较熟练，python sdk没有java scala熟悉，但能写能改。页面也能写一些。因为做的事情很多，这里列出一些关键的擅长的事情：</p>
<p>   1.能够扩展spark source transformer estimator udf 以及UI 支持各类算法 支持python语言跨平台。<br>   2.以前使用过fastText 以及textCNN做文本反垃圾pipline 算法流程。<br>   3.以前做过app流量电量预测使用过特征工程，然后DNN 以及 xgboost 以及 随机森林  然后blend 做喜马拉雅流量电量预测。<br>   4.spark上为了后来使用方便封装了一些常用的套路，支持一些常用的算子 UDF 以及 模式。<br>   5.在用户画像方面尝试过DNN拟合 尝试过LSTM 因为能够熟练用于机器学习框架 深度学习框架，也会写python 所以能够很容易的将python算法代码改成scala spark分布式上运行以及网上开源的各类AI库能够熟练使用运用。<br>   6.特征工程方面也很熟练。<br>   7.了解AI原理，喜欢研究最新的AI成果比如 后面提到的transformers 架构 GAN 强化学习 还是 bert VAE变分，经常自己根据这类的网络设计规则防止过拟合的门结构，残差结构 attention模型 batchNorm 以及激活函数特性来自己设计一些网络去尝试，一直在研究,比如像尝试GAN在kaggle card欺诈样本生成对抗，最后得到的结果在过一次原始的xgboost在样本集上效果挺好。<br>   8.做过分布式日志监控系统，从链式追踪到数据同步以及到最后的数据仓库，从详细表到大宽表到统计表。从spark stream storm 实时计算到 spark hive计算，统计方面从指标变量管理平台到任务调度管理平台，这其中我最擅长自动化，自动代码生成自动模板生成，封装套路<br>   9.风控方面 在喜马拉雅与美团做过变量的计算封装以及数据自动同步，审批框架的抽取封装，审批运维平台管理，频控限流 pie规则引擎（drools动态封装），zk实现的网络分布式设计，自动化的变量计算以及累计服务。<br>  10。做事效率高，不会重复做很多事情，所以在平台架构时候，很多东西全部都是实现自动化的。因为平台架构就我们5个人，但是要做的事情很多，管理排除问题很多，所以很多东西都会自动化封装，即使排查问题，都有一个直观可视化搜索界面，所以页面我也会做，学习东西比较快吧，一个陌生的东西很快就能上手，了解原理。</p>
<h3 id="项目介绍"><a href="#项目介绍" class="headerlink" title="项目介绍"></a><strong>项目介绍</strong></h3><p>   这个项目最近暂时停止开发了，因为剩下大量重复性代码生成包装工作了,（其实也在研究图DAG automl自动构造的方法，看了一些google的paper知道要怎么做了，还有参数动态管理控制的方法也知道怎么做了）有时候在想如果能够实现DAG 拓扑图动态搜索所有可能，自动生成模型，然后对于每一个DAG依赖之间是否可以通过默克尔树来来保证流程完整准确性，进而实现真正的automl 我觉得，真正的automl 应该是不需要用户去拖拉生成DAG的（用户需要知道数据特征工程算法知识，门槛太高了），而应该通过数据以及一些约束 特征工程方式来规划搜索得到最优的DAG流程。以前觉得这个是不太可能的，最近看了google的一些paper发现是可以做到的。</p>
<p>   最近也要开始找工作，上班也有点忙，周末也在研究flink 以及一些newSQL ,个人感觉很多分布式系统 计算 存储 从设计原理思想都有异曲同工，万变不离其宗，我自己打算工作空余，周末空闲时间拆解分布式存储，分布式计算，自己能够深入彻底的从现在理解原理，架构设计思想，到怀疑性能高效，到了解一个分布式（计算/存储/框架）内部每一个个细节，希望不久将来拥有自己能够独立实现一个性能很好的轮子（分布式高性能计算/存储/系统）的能力</p>
<p>   个人从12月底左右空余时间开始想做一个自动化机器学习平台，为什么想做这个平台，一方面是一个学习提升能力项目，一方面是希望能展示自己的能力，另一方面是公司做的东西因为涉及公司机密无法展示，所以想通过这个平台来展示一下自己的能力，因为空余时间有限，周六周末偶尔做一下，所以并没有完整做完，只完成了核心部分，这里就展示一下个人的设计思路链条。</p>
<p>   参考了阿里的PAI ，因为PAI 是中断的，对于每一个节点处理，然后通过airflow类似工作流来管理流程，产生很多中间数据，会依赖很多脚本运行，我的想法是 对于数据读取 数据清洗 数据转换 特征工程 算法模型 超参数优化，这应该是一个pipeline过程，而每一个pipeline 流程可以有上下依赖的关系，这样大的pipeline流程使用airflow来管理依赖，小的pipeline流程直接使用DAG 有向无环图的结构来管理 类似spark 中DAG 管理宽窄依赖使用深度优先遍历来逐步初始化 compose模式搭建积木来构建一个完整流程代码。<br>   首先看看我的设计图</p>
<p>   <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/flowdesigner01.jpg" alt></p>
<p>   从大的方向主要分为四个层：<br>   引擎层、拓扑图层 、UI可视化层、任务调度层<br>   主要原理就是通过对图的构建，通过前端jsplumb可视化技术来实现拖拉生成相关图，然后进行graphml系列化，最后通过DFS 遍历获取每一个节点Node 通过compose模式动态生成相应的引擎代码，交给lts分布式任务系统动态运行job。</p>
<p>   <strong>第一点Graph图构建封装</strong>：是核心类 graph storm中的拓扑图结构，有向无环图DAG 顶点与边组成<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/toplogy%E5%B0%81%E8%A3%85%E9%A1%B6%E7%82%B9%E8%BE%B9.png" alt><br>jgrapht 转为graphml 系列化存储以及jsplumb可视化。<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E6%B5%81%E7%A8%8Bjsplumb%E4%BB%8Egraphml%E8%BD%BD%E5%85%A5%E5%8F%AF%E8%A7%86%E5%8C%96.png" alt></p>
<p>演示：<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E8%BF%9B%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%9B%E5%BB%BA.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E8%BF%9B%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%9B%E5%BB%BA1.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E8%BF%9B%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%9B%E5%BB%BA2.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E8%BF%9B%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%9B%E5%BB%BA%E8%8A%82%E7%82%B9%E5%8F%82%E6%95%B0%E5%80%BC%E8%AE%BE%E7%BD%AE.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E8%8A%82%E7%82%B9%E7%B1%BB%E5%88%AB.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E8%8A%82%E7%82%B9%E5%88%97%E8%A1%A8.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E8%8A%82%E7%82%B9%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E6%B5%81%E7%A8%8B%E9%93%BE%E6%8E%A5%E4%B8%8E%E4%BF%9D%E5%AD%98.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E6%B5%81%E7%A8%8B%E5%88%9B%E5%BB%BA.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E6%B5%81%E7%A8%8B%E4%BF%9D%E5%AD%98.png" alt></p>
<p><strong>第二点 spark datasource  transformer estimator封装</strong></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E9%A1%B9%E7%9B%AE%E6%A8%A1%E5%9D%97.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E6%95%B4%E5%90%88%E4%B8%80%E4%BA%9Bspark%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%BA%9B%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/python%E4%BB%A3%E7%A0%81%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E5%B0%81%E8%A3%85.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/pyspark%E4%B8%80%E4%BA%9B%E6%B5%8B%E8%AF%95%E4%BB%A5%E5%8F%8A%E7%89%B9%E5%BE%81%E5%B0%81%E8%A3%85.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/python_template.png" alt></p>
<p>在spark mlib基础<br>整合了spark 一些相关库<br>mmlspark bigDL H2O xgboost lightgbm 另外自己也实现了一些estimator 采样 异常点 以及除去NAN 去除重复，除去高基数，编码等等,自定义扩展了一些数据源比如更好的hbase 阿里云的tablestore 以及 tushare查询A股股票数据。另外因为pyspark py4j关系，所以可以通过py4j 实现python相关方法比如bert 文本特征提取然后scala pyspark可以直接调用，原理是因为 在pyspark中把相关bert处理 注册为UDF，但是这个UDF 是python UDF ，所以在scala 使用时候需要实现相应的转换 转换成UserDefinedPythonFunction 就可以在scala中使用了，同时因为新版本支持pandas_udf 所以可以支持sklearn相关包一些transformer可以使用了。<br>这里我定义了一个tushare 查询A股股票以及bert模型支持（内存占用太大，在解决）。<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/tushare%E6%94%AF%E6%8C%81.png" alt><br>比如fastText预训练模型可以直接pyspark 方式 也可以直接bigDL 来加载，在工作中偶尔会用到，通过bigDL keras 嵌入层 加载 dense层加载方式可以实现estimator， 还有很多比如上下采样实现，还有一些特征工程套路，我就直接使用UDF来定义（pyarrow支持pandas）就好了。<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/fastText01.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/fastText02.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/bert%E6%A8%A1%E5%9E%8B.png" alt></p>
<p>可惜的是bert展示只能本地运行，因为线上依赖预训练模型，需要通过spark addFile addJar 加入来获取地址 spark.get获取相关模型来分布式读取预训练模型，所以这类很多相关的先见依赖的工作。比如pip setup构建python本地包，以及拷贝这类文件工作我后面打算用docker 来管理，使用K8S来自动编排.平时时间不多这一步还没完成 K8S也在研究。</p>
<p>第三部就是 动态代码生成：<br>通过图的遍历，来获取节点，然后搭积木方式来生成相关xml 以及相关代码。这里我完整实现了spring的xml的 构建生成。因为大部分时间在实现spark那些datasource estimatro transformer 自定义udf所以spark的代码只实现了部分。<br>这是spring integation封装<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/springintegration%E5%B0%81%E8%A3%85.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/springintegration%E5%B0%81%E8%A3%852.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E5%BC%82%E6%AD%A5%E5%AE%A1%E6%89%B9%E6%B5%81%E7%A8%8B%E5%B0%81%E8%A3%85.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/%E6%B5%81%E7%A8%8B%E5%9B%BE%E7%A7%AF%E6%9C%A8%E6%9E%84%E5%BB%BA.png" alt><br>对于spark引擎封装主要<br>实现链式<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/python_template.png" alt><br>比如<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/pyspark%20%E9%93%BE%E5%BC%8F.png" alt><br>对于pyspark<br>通过freemarker引擎 来生成spark代码 直接填充pyspark代码链式块就可以了，这一部分因为节点太多，没有完整完成。</p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/spark%E6%9E%84%E5%BB%BA1.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/sparkgoujian2.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/spark%20%E6%9E%84%E5%BB%BA3.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/spark%E4%B8%80%E4%BA%9B%E5%B8%B8%E7%94%A8%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E6%96%B9%E5%BC%8F.png" alt></p>
<p>为了方便sdk使用采用<br>stepBuilder构建模式进行约束<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/stepBuilder%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%BC%8F.png" alt></p>
<p>因为spark 的没有封装完，这里演示 spring的版本：<br><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/spring01.png" alt></p>
<p><img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/spring2.png" alt></p>
<p>总结：<br>时间原因过多浪费在页面以及spark 算法 特征工程节点的工作上，以及研究k8s自动编排上并没有完成整个项目，所以这里只演示展示了个人项目的部分代码。个人比较擅长大数据 机器学习算法工程方面。可以熟练使用各类算法 以及深度学习算法在spark中部署应用。能够python scala java 混合开发。能够把python sklearn 相关包大数据工程化。上面这个项目主要是自己学习项目，一方面可以通过这个项目来了解使用各类算法，另外一方面是希望能做一个好的开源项目。后面我打算实现几个方面：<br>  1.引入K8S自动编排<br>  2.实现页面配置python 代码以及Java spi模块插件，动态定义UDF 来自定义数据节点<br>  3.实现BigDL的研究想实现keras 深度学习模型自定义<br>  4.整合成功bert 还有一些常用的深度学习模型 一些常用特征工程套路到spark工具包中。<br>  5.引入sqlparser 扩展SQL引擎使其可以通过SQL 实现机器学习的更便捷查询语法。<br>  6.支持更多可视化比如现在整合的handspark只支持简单的直方图。箱图<br>  7.研究mleap 来尝试实现发布部署sdk 然后通过flink来部署上线进行相关预测。<br>  8.通过个人学习项目来尝试研究图来尝试元学习以及图 来实现一些automl。学习更多的算法知识。</p>
<h2 id="（PS-个人感言：）"><a href="#（PS-个人感言：）" class="headerlink" title="（PS 个人感言：）"></a>（PS 个人感言：）</h2><p>个人成长最大的应该是喜马拉雅吧，跟随从平台架构从无到有，比如推拉模式的分布式配置中心,比如基于zk paxso算法以及raft算法的etcd等思想原理的分布式框架的架构设计，在进一步封装的网关系统，分布式日志系统，数据仓库搭建，比如基于docker以及后来的k8s的cmdb发布平台，spark 流与离线日志统计报警监控，日志管理搜索，storm增量同步，爬虫舆情webdriver 分布式代理抓取，算法机器学习风控，文本评论反垃圾的运用，图算法LPA 欺诈账号，刷播放账号的寻找。opentsdb图表统计可视化，后来来到美团发现整个公司的技术栈都很熟悉了解，很多曾经参与做过。很感谢喜马拉雅，个人学习能力很强，特别喜欢多思考一步，每次都会反省自己，喜欢碎片化学习，总结提炼自己的架构经验算法工程能力，特别希望有这样一个平台表现自己。现在在金融部门虽然，在这里我给rcdata实现了自动化数据同步，指标变量管理，自动化审批平台，后台运维管理，但是并没有体现自己在架构大数据算法工程上的能力，风控偏向第一代初期的规则引擎的相关操作，我本来期望来到这里能够实现一套从用户申请，然后数据指标计算，自动化特征工程，从实时宽表到离线宽表，从数据源到数据输出，从批量到实时，从DAG流程节点管理到算法模型pipeline,最后到强化学习自动反馈机制自动更新模型也想尝试引入ftrl 用一些深度学习来做尝试，最后统筹数据接入到图计算 图谱一些AI算法的尝试上形成一条完整的链，从数据接入 模型预测，图推理，数据统计 形成一个完整的链路环。用户每笔申请，每一笔还款，用户上次还款这次还款时间，等等 用户各类行为，都能够自动的生成各类维度的指标变量（自己总结的特征工程变量套路）甚至每天不同时间的申请量 都可以采用ARIMA以及相应的prophet ,AI相关前馈网络 LSTM transfomers结构 HTT等时间系列算法来预测每天不同时间的申请量，来进行相应的限流控制，预测评估。最后做到AIOps ，下面这个项目就是我平时周六周日做的，原因说过了，其实最开始初衷是想实现 从数据源到数据输出，大数据算法AIOps化。能够应用到股票其他相关预测中，最后通过强化学习自动调整深度学习网络，设计网络来做到自我强化，然后可以生成SDK 可以手机微信号监控可视化展示，能够自动模型部署上线进行各类预测。 </p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/07/image-model/">2019-05-08 图像方面的一些尝试</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-07</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/图像方面的一些探索笔记/">图像方面的一些探索笔记</a></span><div class="content"><p>我其实以前做过很多算法学习尝试，跟我博客写的那样，其实也偶尔碎片时间学习一下最近的paper算法，我学习能力比较强，虽然精力有限，但是我觉得知道很多广的知识面，你可以有很多思路解决各类无边界跨领域的问题，其实做大数据，像spark core<br>stream mlib graphx ,core其实就是需要大数据方面掌握 离线的计算，设计，存储一些知识，stream流计算实时计算（flink storm） mlib使用就是需要你掌握机器学习 深度学习算法你才能使用，graphx 就需要你掌握图论以及一些LPA deepwalk GNN图深度学习 GMN 图像匹配网络 pregel模式 社区检测 强连通图，跟进一步可能就是知识图谱RDF 主谓宾，所以感觉要研究的东西还有很多，所以我觉得我一直摸透spark 每一个细节 spark里面涉及的相关知识，设计细节，就有很多值得学习的。废话不多说<br>    最近晚上空余时间在查阅google一些paper 怎么实现动态生成深度学习网络（当然是一些现有的通用模块），研究了一些图像CNN 最新的改进，从alexNet 到 vgg 到googleNet<br>到denseNet 到 XInception  到 capusleNet （胶囊网络）到 MobileNet  到 ShuffleNet 以及到最新把transformer 注意力机制应用到CNN上的NleNet，从 group conv 分组卷积 到 空洞卷积 到 local conv 卷积 到 1x1 3x3 inception 链接结构 还有 concat add的 残差skip connection结构 bottleneck结构 到 depthwise 结构 sufflechannel改进,到注意力机制结构，到多遍分路结构，大体来说 2个方面 1方面 从卷积角度来说改进更小的参数，更灵活的变动 另一方面 从网络总体角度 更好的收敛，更好的模式。<br>所以后面我打算 先做一个基本模型，然后尝试着 套上 group conv 残差bottleneck残差结构 空洞卷积<br>depthwise卷积 和 pointwise卷积 多路结构 最后 结合GAN生成网络来实现一个模型来试试 。</p>
<p>2019年5月7号晚上：<br>  我在网络上下载了一个keras 的项目作为baseline 来修改，这是一个训练衣服颜色 以及 种类的模型，我刚搭建起来 运行训练了一下：</p>
<p>  这个模型挺有意思，他有2个输出，为啥用它就是 因为这样就是一个DAG，而不是一个最常见的系列方式，可能有多个输入或者多个输出，他的输出一个是颜色 一个是种类，<br>  先看看训练数据源<br>  就是一些物品 标记了颜色以及种类，可以用 kaggle的水果数据来做训练更大更齐全<br>  <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/WX20190508-105525%402x.png" alt><br>  <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/WX20190508-105548%402x.png" alt><br>  原始的网络设计<br>  <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/keras_multi_output_fashionnet_top.png" alt><br>  <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/keras_multi_output_fashionnet_bottom.png" alt></p>
<p>  我下载后重新训练了一下，很快本地就训练完了,执行预测效果如下：<br>  <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/WX20190508-000455%402x.png" alt></p>
<p>  我们可以先看看它内部代码，无非就是卷积池化卷积，我们先不动他链接关系：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">x = Activation(&quot;relu&quot;)(x)</span><br><span class="line">x = BatchNormalization(axis=chanDim)(x)</span><br><span class="line">x = MaxPooling2D(pool_size=(3, 3))(x)</span><br><span class="line">x = Dropout(0.25)(x)</span><br><span class="line"></span><br><span class="line"># (CONV =&gt; RELU) * 2 =&gt; POOL</span><br><span class="line">x = Conv2D(64, (3, 3), padding=&quot;same&quot;)(x)</span><br><span class="line">x = Activation(&quot;relu&quot;)(x)</span><br><span class="line">x = BatchNormalization(axis=chanDim)(x)</span><br><span class="line">x = Conv2D(64, (3, 3), padding=&quot;same&quot;)(x)</span><br><span class="line">x = Activation(&quot;relu&quot;)(x)</span><br><span class="line">x = BatchNormalization(axis=chanDim)(x)</span><br><span class="line">x = MaxPooling2D(pool_size=(2, 2))(x)</span><br><span class="line">x = Dropout(0.25)(x)</span><br><span class="line"></span><br><span class="line"># (CONV =&gt; RELU) * 2 =&gt; POOL</span><br><span class="line">x = Conv2D(128, (3, 3), padding=&quot;same&quot;)(x)</span><br><span class="line">x = Activation(&quot;relu&quot;)(x)</span><br><span class="line">x = BatchNormalization(axis=chanDim)(x)</span><br><span class="line">x = Conv2D(128, (3, 3), padding=&quot;same&quot;)(x)</span><br><span class="line">x = Activation(&quot;relu&quot;)(x)</span><br><span class="line">x = BatchNormalization(axis=chanDim)(x)</span><br><span class="line">x = MaxPooling2D(pool_size=(2, 2))(x)</span><br><span class="line">x = Dropout(0.25)(x)</span><br><span class="line"></span><br><span class="line"># define a branch of output layers for the number of different</span><br><span class="line"># clothing categories (i.e., shirts, jeans, dresses, etc.)</span><br><span class="line">x = Flatten()(x)</span><br><span class="line">x = Dense(256)(x)</span><br><span class="line">x = Activation(&quot;relu&quot;)(x)</span><br><span class="line">x = BatchNormalization()(x)</span><br><span class="line">x = Dropout(0.5)(x)</span><br><span class="line">x = Dense(numCategories)(x)</span><br><span class="line">x = Activation(finalAct, name=&quot;category_output&quot;)(x)</span><br><span class="line"></span><br><span class="line"># return the category prediction sub-network</span><br><span class="line">return x</span><br></pre></td></tr></table></figure>
<p> 我们试着尝试使用depthwise卷积替代现有的卷积，后面几天尝试再改链接方式 使用skip connection看看。首先看看depthwise 卷积概念，就是深度上每个channel进行卷积得到featureMap，之后再用1x1卷积来进行卷积， 可以大大减少参数，<br> <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/20170825221844235%20%281%29.bmp" alt></p>
<p> 我们直接使用官方仓库models 引入mobileNet里面看看实现就可以了<br> 官方仓库地址</p>
<p><a href="https://github.com/fchollet/deep-learning-models" target="_blank" rel="noopener"> https://github.com/fchollet/deep-learning-models</a><br> 看到了他的实现了。我们现在参考官方库修改一下我们的</p>
<figure class="highlight plain"><figcaption><span>CONV </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">x = DepthwiseConv2D((3, 3), padding=&quot;same&quot;, use_bias=False)(x)</span><br><span class="line">x = BatchNormalization()(x)</span><br><span class="line">x = Conv2D(32, (1, 1), padding=&quot;same&quot;, use_bias=False)(x)</span><br><span class="line">x = Activation(&quot;relu&quot;)(x)</span><br><span class="line">x = BatchNormalization(axis=chanDim)(x)</span><br><span class="line">x = MaxPooling2D(pool_size=(3, 3))(x)</span><br><span class="line">x = Dropout(0.25)(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># x = DepthwiseConv2D(32, (3, 3), padding=&quot;same&quot;)(x)</span><br><span class="line"># x = Activation(&quot;relu&quot;)(x)</span><br><span class="line"># x = BatchNormalization(axis=chanDim)(x)</span><br><span class="line"># x = MaxPooling2D(pool_size=(3, 3))(x)</span><br><span class="line"># x = Dropout(0.25)(x)</span><br><span class="line"></span><br><span class="line"># (CONV =&gt; RELU) * 2 =&gt; POOL</span><br><span class="line">x = DepthwiseConv2D((3, 3), padding=&quot;same&quot;, use_bias=False)(x)</span><br><span class="line">x = BatchNormalization()(x)</span><br><span class="line">x = Conv2D(64, (1, 1), padding=&quot;same&quot;, use_bias=False)(x)</span><br><span class="line">x = Activation(&quot;relu&quot;)(x)</span><br><span class="line">x = BatchNormalization(axis=chanDim)(x)</span><br><span class="line">x = DepthwiseConv2D((3, 3), padding=&quot;same&quot;, use_bias=False)(x)</span><br><span class="line">x = BatchNormalization()(x)</span><br><span class="line">x = Conv2D(64, (1, 1), padding=&quot;same&quot;, use_bias=False)(x)</span><br><span class="line">x = Activation(&quot;relu&quot;)(x)</span><br><span class="line">x = BatchNormalization(axis=chanDim)(x)</span><br><span class="line">x = MaxPooling2D(pool_size=(2, 2))(x)</span><br><span class="line">x = Dropout(0.25)(x)</span><br></pre></td></tr></table></figure>
<p>网络我并没有采用MobileNet2 的跟残差结合的模块，也没有使用shuffleNet的chanelsuffle的方式，就是采用MobileNet1的直接方式。我修改了其中部分代码，注释的是源代码</p>
<p> <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/WX20190508-012726%402x.png" alt></p>
<p> 下面来训练看看效果</p>
<p> 开始训练<br> <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/WX20190508-030948%402x.png" alt><br> epoll迭代中<br> <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/WX20190508-032313%402x.png" alt><br> 最后训练完成：</p>
<p> <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/WX20190508-103642%402x.png" alt></p>
<p> 最后进行预测：<br> <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/WX20190508-104033%402x.png" alt></p>
<p> 结果比较：<br> 好像更稳定了些,准确率低了一些<code></code><br> <img src="http://autometis.oss-cn-shanghai.aliyuncs.com/automl/WX20190508-104226%402x.png" alt></p>
<p> 明天后天晚上下班到家后，我再在spark 上scala实现一下,在spark 上 如果使用scala 可以使用dl4j 或者 bigDL 或者 使用python 可以直接复用上面的代码，下面贴出对比</p>
<figure class="highlight plain"><figcaption><span>com.intel.analytics.bigdl.nn.keras.&#123;Input, Model&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">import com.intel.analytics.bigdl._</span><br><span class="line">import com.intel.analytics.bigdl.nn.Graph.ModuleNode</span><br><span class="line">import com.intel.analytics.bigdl.numeric.NumericFloat</span><br><span class="line">import com.intel.analytics.bigdl.nn._</span><br><span class="line">import com.intel.analytics.bigdl.utils.Shape</span><br><span class="line"></span><br><span class="line">object FashionNet &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def build_category_branch(height:Int,weight:Int,categoryNum:Int,inputs:ModuleNode[Float])=&#123;</span><br><span class="line">    import com.intel.analytics.bigdl.nn.keras._</span><br><span class="line">    import com.intel.analytics.bigdl.utils.Shape</span><br><span class="line"></span><br><span class="line">    var x = Reshape(Array(1, height, weight)).inputs(inputs)</span><br><span class="line">    x = Convolution2D(32, 3, 3).setName(&quot;conv1_3x3&quot;).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x=MaxPooling2D((3,3)).inputs(x)</span><br><span class="line">    x=Dropout(0.25).inputs(x)</span><br><span class="line">    x = Convolution2D(64, 3, 3).setName(&quot;conv2_3x3&quot;).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x = Convolution2D(64, 3, 3).setName(&quot;conv3_3x3&quot;).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x=MaxPooling2D((2,2)).inputs(x)</span><br><span class="line">    x=Dropout(0.25).inputs(x)</span><br><span class="line">    x = Convolution2D(128, 3, 3).setName(&quot;conv4_3x3&quot;).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x = Convolution2D(128, 3, 3).setName(&quot;conv5_3x3&quot;).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x=MaxPooling2D((2,2)).inputs(x)</span><br><span class="line">    x=Dropout(0.25).inputs(x)</span><br><span class="line">    x = Flatten().inputs(x)</span><br><span class="line">    x = Dense(256).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x=MaxPooling2D((2,2)).inputs(x)</span><br><span class="line">    x=Dropout(0.5).inputs(x)</span><br><span class="line">    x = Dense(categoryNum).inputs(x)</span><br><span class="line">    x = Activation(&quot;softmax&quot;).setName(&quot;category_output&quot;).inputs(x)</span><br><span class="line">    x</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  def build_color_branch(height:Int,weight:Int,numColors:Int,inputs:ModuleNode[Float])=&#123;</span><br><span class="line">    import com.intel.analytics.bigdl.nn.keras._</span><br><span class="line">    import com.intel.analytics.bigdl.utils.Shape</span><br><span class="line"></span><br><span class="line">    var x = Reshape(Array(3, height, weight)).inputs(inputs)</span><br><span class="line">    x = Convolution2D(16, 3, 3).setName(&quot;conv1_3x3&quot;).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x=MaxPooling2D((3,3)).inputs(x)</span><br><span class="line">    x=Dropout(0.25).inputs(x)</span><br><span class="line">    x = Convolution2D(32, 3, 3).setName(&quot;conv2_3x3&quot;).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x=MaxPooling2D((2,2)).inputs(x)</span><br><span class="line">    x=Dropout(0.25).inputs(x)</span><br><span class="line">    x = Convolution2D(32, 3, 3).setName(&quot;conv3_3x3&quot;).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x=MaxPooling2D((2,2)).inputs(x)</span><br><span class="line">    x=Dropout(0.25).inputs(x)</span><br><span class="line">    x = Flatten().inputs(x)</span><br><span class="line">    x = Dense(128).inputs(x)</span><br><span class="line">    x= Activation(&quot;relu&quot;).inputs(x)</span><br><span class="line">    x=BatchNormalization().inputs(x)</span><br><span class="line">    x=Dropout(0.5).inputs(x)</span><br><span class="line">    x = Dense(numColors).inputs(x)</span><br><span class="line">    x = Activation(&quot;softmax&quot;).setName(&quot;color_output&quot;).inputs(x)</span><br><span class="line">    x</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  def build(height:Int,width:Int, numCategories:Int, numColors:Int)=&#123;</span><br><span class="line">    val input = Input(inputShape = Shape(height, width, 3))</span><br><span class="line">    var category_branch=build_category_branch(height,width,numCategories,input)</span><br><span class="line">    var color_branch=build_color_branch(height,width,numColors,input)</span><br><span class="line"></span><br><span class="line">    Model(</span><br><span class="line">      input = input,</span><br><span class="line">      output=Array(category_branch,color_branch)).setName(&quot;fashionNet&quot;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> PS(我最近有个想法想要去尝试一下就是多个GAN模式 来整合yolov3 以及 mobileNetV2 实现对象侦测以及图像识别，然后多个GAN之间实现拓扑图，实现一个可以配置的超级GAN 网络。为什么有这个想法是因为GAN网络 一直依赖由于他的先天性，这种纳什均衡竞争关系，跟我们人一样不断有竞争对抗才会成长，最后看到的效果也一直非常好，<br>所以才有这个打算。) </p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/08/cappaxso/">谈谈自己对分布式一致性算法的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-08</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/paxso-raft-gossip-qurorum/">paxso raft gossip qurorum</a></span><div class="content"><p>说起分布式一致性算法，就要讲到CAP原则，我们在多机器之间、多副本之间如果不需要强一致性，直接读写 主从（半）异步同步就行，但是总是有一些场景，需要保证每次获取到结果都是一致性的，比如资源注册管理一些metadata schema信息，一致性状态，各个副本或者节点之间操作严重依赖彼此数据一致，这时候需要保证一致性或者最终一致性。总体来说现有一些zookeeper consul etcd atom一些开源框架实现了这些一致性算法。<br>这类算法有个基本思想就是（NWR除外）对于每台机器相同的初始状态，或者某个时间相同的初始状态，如果经过的状态系列变更相同最后的结果一定是相同的，同样对于相同的变更系列分为2种一种是基于过程的变更系列 这叫做复制状态机，比如paxso raft 记录变更的前后日志信息，还有一种是基于结果的变更系列 这叫做primary-backUp。比如zookeeper ZAB基于数据结果的变更系列。在了解分布式一致性算法前，请先知道一个问题什么是2PC，网上文字很多这里就不说了，其实网上很多paxso 算法太多抽象难懂，我也看过很多，但是太过晦涩难懂，下面就用我理解的方式来说吧，我以前也没怎么搞懂，后来看了一下mutipaxso一些框架实现的源码，渐渐的懂了。首先我们知道 2个角色，一个提案者 一个议会者，<br>他是怎么做的呢？ 让我们想象一个场景，读书时候，学校搞庆祝活动，<br>我们一大群人去操场抢位置，每个人都想抢到一个靠前的座位，比如 1–20 20-30 30-40 ，但是实际上很多空位，有可能一部分同学去厕所了，去组织表演了，这时候来了一个同学（请求）我先看中1号座位，所以他会做这些事情， 我想周边的在的人询问 这个1号座位 有人占了吗？但是每个人知道的可能不一样，因为也许别人没有关注这些，根本不知道，但是对于每个人来说他总不知道，或知道<br>我知道哪些之前位置座位有人占了，我知道几号座位刚刚谁占了，但是我们不可能记住每一个座位，但是我们知道 1 2 3 4 5 6 7 这数字顺序是不变的，如果我们能按照这个顺序来占的话，一步步确认保证，是不是从1 、、、到N被占信息如果都确定了，只需要知道 某个N就行了，<br>好，下面我们做2次询问来看看能不能占当前这个座位N 。<br>   第一阶段：<br>   A.我们发起第一次询问，问旁边的人，座位N 有人占了吗？ 所有人收到消息的根据自己记得信息答复，比如他记得N之前的之前的座位都被占了但是反正当前N没被占，那么他就答复 可以占ack，否则不答复，同时这个答复的人心里打了小九九，知道了有人对N座位有人想要占了，<br>   这就对应paxos的</p>
<p>   <strong>Prepare阶段：<br>Proposer选择一个提案编号n并将prepare请求发送给 Acceptor。<br>Acceptor收到prepare消息后，如果提案的编号大于它已经回复的所有prepare消息，则Acceptor将自己上次接受的提案回复给Proposer，并承诺不再回复小于n的提案。</strong><br>其中座位号是编号N，要记录的信息是<br>（minProposerId,acceptId,acceptValue）是其他同学心里记得占了的信息 只需要记得哪个之前的座位占了就行，这样同学来问，我就知道该回还是不回了。<br> 第二阶段：<br> B.当超过半数的同学都回我，告诉我这个位置现在还没有占，应该可以占。这时候我肯定去确认并试图占下来，我就告诉其他同学，我要宣告我要占这个座位了，如果这个我们去占的时候，还没人占，好，那我们占下来，其他同学也知道了，心里记下了，座位N已经做了谁啊，N之前的都已经确定了，但是询问发现假如这期间另外一同学已经把位置占了，这时候我们知道泡汤了，我们只能默默的让给他，自己知道N已经被谁做了，但是我们可以去抢N+1的座位吧，重新进行询问。</p>
<p> 这就是paxso 第二个过程</p>
<p> <strong>Accept阶段：<br>当一个Proposer收到了多数Acceptor对prepare的回复后，就进入批准阶段。它要向回复prepare请求的Acceptor发送accept请求，包括编号n和根据prepare阶段决定的value（如果根据prepare没有已经接受的value，那么它可以自由决定value）。<br>在不违背自己向其他Proposer的承诺的前提下，Acceptor收到accept请求后即接受这个请求。</strong></p>
<p>总体来看就是通过座位 1 &lt; 2 &lt; 3 &lt;4 &lt;n 这种有顺序的简单数字顺序，我们来放每次接收到的变更命令请求 一个坑一个坑的往里面放，这有什么好处呢，因为这样有顺序，一旦多数投票节点已经确定知道了某个K 坑已经被占了，代表这这些人知道原来之间1 …到k-1是被占了的，这样不管怎样，也就是说2次询问，不管中间那些节点丢失了链接失联了，他只要发起1….k-1坑的询问，<br>通过多数投票的这些节点知道2次交互，<br>每次都肯定返回同样的结果,而且也能得到原来我不知道的那些坑已经有人占了啊的信息，这相当于查询，这样那些不知道的失联节点可以通过这样询问，同步更新自己知道的信息，最终保证一致性。</p>
<p>这样应该清晰了吧，这是我的理解，其实这basic paxso 有2个问题，1.1个是活锁问题，第二个是2次询问交互时间问题，所以在mutipaxso 问题上就改进了，活锁是因为多个同学可能同时都要抢座位并发问题，导致有些同学一直没抢到一直问下一个下一个座位，问的都快哭了，所以选出一个班主任leader，通过班主任来统一进行询问安排座位，2.2次交互时间问题，因为统一了班主任来问了，所以班主任在问的时候不要每次都问，只需要说我要询问安排座位了，然后就问这个作为能不能做，那个能不能做就行。</p>
<p>今天回家有点晚了 ，先说到这里吧。<br>后面有时间补充raft zab 一些改进的思想，其实跟mutipaxso很像。就像paxso 里面的同学记得n之前的座位被占了信息，在raft里面就是日志以及commitId,下次有时间在补充。</p>
<p>今天接着讲一下 我对Raft的理解，2013 年raft算法开始出现，其实Raft 有点类似上面的paxos的mutiPaxso改进，mutiPaxso没有要求保证同一个时间同一个任期只有一个leader,但是Raft算法保证了，那应该怎么保证呢？ 我们回想一下paxso上面讲的那个N 以及 N-1 之前的座位确认，我们就很快想到，我们把任期也做成一个N N-1之前的座位号确认就行，在Raft 他叫做term ，同时以前的同学占座的那个N 是我们最终要每个同学都一致认同的已知信息，这个N 在Raft 里面就是他的日志的commitIndex ,所以思想很相似吧，有时候你把raft当做paxso的改进也未尝不可。对于Raft来说保证每一次只有有一个leader 所以他就有一个选举阶段。选举跟上面paxso一样，肯定要那个term最大，知道的信息最多（commitIndex最新）的人才能当选班主任啊，<br>啊，同样成为leader后 班主任就要同步我知道的信息给现场同学或者一些刚到的同学，所以有一个消息同步阶段，这里想象班主任跟同学同步信息，怎么做，肯定是说我这边到N的座位都是已经确定的，同学心里想我只记得到K的座位都是确定了的，那么我把班主任知道的K-N之间的信息记下来，这样我也知道到N的所有座位情况了，如果班主任跟我知道座位做的人不一样，那肯定是我错了，我就删除更新记下班主任知道的情况，其他的各类情况反正根据情况考虑就行了，这就是第二个阶段 ，假如下午时候班主任突然生病住院了，是不是会有代课老师来负责安排这些工作，这时候 就是第三阶段重新选举并恢复阶段，代课老师成了代理班主任，这时候肯定给所有同学做一个宣告，广播，告诉同学们，最近这段时间我代理班主任职务来负责，这时候同学都记得了，term+1了，最新的记忆里面都知道了最新的班主任是谁，但是实际上raft 跟 现实有点不一样，假如班主任又突然好了回来了，这时候本应该是他回来当班主任，但是在raft里面就不一定了，因为当前的是term+1比班主任term要大，班主任成不了leader了，只能等代理班主任当完代理完走，下次再去选举了，所以如果按照这个场景来分析，其实可以实现leader节点群，follower节点群，leader节点就是班主任有其他备选的一些，也许做做投票议会，优先级权利比follower节点更高，kafka里面这有点类似ISR，只有ISR（班主任病了时候有资格代理班主任的）才能当选班主任。就讲这么多吧。ZAB 跟Raft很类似。大家可以相同的理解就行。但是里面同步消息有点不太一样。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/09/spark理解/">我理解的分布式计算系统</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-09</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/spark-storm-flink/">spark storm flink</a></span><div class="content"><p>接触spark从2015年出现火爆开始到至今。其中接触并使用过storm 使用过现今比较流行的flink，还记得2014年底，我在一家医疗分析的创业公司时候，工作需要，研究并接触到了并发批处理，那时候mapreduce开始出现，接触到了disrupter并发框架，接触到了netty网络通讯一些东西，那时候想是不是通过netty网络通讯保持多台机器并发并行的执行并发框架是不是可以实现分布式计算呢，那是我第一个初步的理解，后来从写代码到熟悉api 到了解一些细节。并心中对spark storm 到 flink 一些归纳总结思考后，我提炼了一些自己的认识。下面说说我眼中的分布式计算。<br>   在传统高并发 有多线程  多进程，比如多线程共享内存方式，多进程消息传递方式，到充分利用特性，多进程并行，多线程消息传递机制，进而到分布式多机器多进程多线程多协程，所以出现了各类分布式计算框架，我总结的大体分类几类：第一类 是基于mapreduce 算子拓扑图的 比如mapreduce storm flink spark 第二类 基于通讯机制 消息之间或实现共享内存共享线程等 ，通讯调度计算的 这一类有ignite xgboost ps-servier 参数服务器这些 第三类基于 图 节点传播pregel模式的 ，大体从数据来说 可以从图的 顶点和边来考虑。一类数据比较小 所以数据是边 顶点是算子 比如storm flink,一类数据较大 所以数据是顶点，算子是边。<br>   大体来说 现在第一类算子拓扑图的分布式计算大体模式是通过抽象出操作作为一些算子，算子之上封装成一些table schema dataset sql一些抽象，对于底层算子之间 只有shuffle 才会进行通讯其他之间不通讯，而算子之间通过有向无环图DAG进行管理划分，对于每一个Job 大体都是 分布式存储抽象层<br>任务调度分配层 网络通讯层 资源管理隔离层，动态跟踪监控层，然后为了保证一致性 通过分布式快照算法来实现流程数据一致性流程完整性。这个设计思想 在其他分布式存储 分布式中间件中很相似也有异曲同工之妙。最近看到flink 开始采用akka 消息通讯机制来替代原来的共享内存锁机制，如果后面flinkml需要支持深度学习的化，也许flink akka实现会向第一类allreduce机制来兼容。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/15/distributeChandy-Lamport/">我理解的分布式快照算法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-15</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/chandy-lamport快照算法/">chandy-lamport快照算法</a></span><div class="content"><p> 对于分布式计算系统，分布式系统数据流动变化时候，往往需要记录一个全局状态也就是快照，以便系统崩溃时候能够通过快照得到每个节点本地快照并恢复。想象一个场景，城市交通四通八达，有很多下水道，蚂蚁是怎么找到并记录道路状况的呢？每一条道路派一支蚂蚁去探路，那如果我们要拍一张照片，全局的快照，但是我们一次性拍不下全部，那我们怎么做呢？我们就每一条分支路径拍一张快照，那么所有的本地快照就可以组成一张全局快照，那每一张本地快照改记录什么呢？对于分布式系统，有是数据从边流向节点，或者操作从边流向节点，进程之间也是一样，那么怎样保证记录快照呢？ 我们这样想，对于当前节点状态我们肯定是要记录的，对于当前节点输入的也是要记录的，输出呢？不需要，因为我们可以通过节点状态以及输入可以得到输出，所以只需要记录状态以及输入就行，但是输入什么时候该记录呢，我们拍照肯定是拍照点之前快到拍照这个点的数据，就像漫画里面一个个慢动作，最后我们眼中看到的是慢动作组成的动作，但是我们快照就是一个个慢动作的帧，是我们最后眼镜拍照那个状态之前的帧<br>    下面说一下<br>     <strong>Chandy Lamport 算法<br>这个算法把P进程-&gt;Q进程数据交互分为四个部分，其中因为每个节点时间上的不一致，所以采用了一个marker 来记录什么时候该拍每一个节点的快照，所以分为4个过程。也就是2个初始态 2个中间态，</strong><br>如果当进程拥有marker状态记为s1,进程未拥有token状态记为s0。</p>
<ul>
<li>1.P进程开始发送marker 到Q进程 此时 P Q  应该为[ p：s1 q：s0] 这时候相当于漫画我们开始要画第一帧，此时人物 环境状态 一张静态图。</li>
<li>2.P已经发送了marker了在传输的channel边上 即将到Q进程</li>
<li>这时候 P Q 应该为 [ p：s0 q：s0 ] 哈哈，此时想象变化的是marker的channel吧，这时候相当于漫画我们开始画第二帧 第二帧 第几帧的中间过程了。多张动画帧</li>
<li>3.进程Q收到marker marker已经到了Q ，这时候相当于我们动画里面 一个片段。</li>
<li>如果有双向的场景，我们倒退场景，顺序 逆序</li>
<li>4.这时候进程Q 发送marker到P</li>
<li>同样有[ p：s0 q：s0 ] </li>
<li>5.这时候进程P 收到了来自Q的marker 同样有 [ p：s1 q：s0 ]<br>那么他怎么快照呢？<br><strong>实际过程很像： 漫画从手绘到动画的过程</strong>。</li>
</ul>
<p><strong>我们要快照时候相当于画动画初始帧，但是这个帧有很多后续变化，所以<br>第一步 先记录初始帧状态，然后当记录成功后发送一个marker拍快照的标记给后续所有帧，<br>这时候遇到后续帧会有如下情况，<br>如果后续动画帧 我们没有记录下来，那么我们要记录当前这帧，同时，我们知道这之前的所有帧已经记录好了，如果实际上发现这一帧已经记录了，说明后续动画有从当前帧开始变化了，下一个片段了，这时候是不是要记录从从上一个片段记录帧完成后 到 这个片段开始记录帧marker之前所有的帧，也就是channel信息</strong></p>
<p>下面是原论文信息：</p>
<p>//（1）（2）<br>begin<br>         p  records its state;<br>end<br>then<br>        p sends one marker along c after p records its state and before p sends further messages along c.<br>//（3）（4）<br>if q has not recorded its state then<br>      begin<br>                 q records its state;<br>                 q records the state c as the empty sequence<br>      end<br>else<br>       q records the state of c as the sequence of messages received along c after q’s state was recorded and before q received the marker along c.</p>
<p>晦涩难懂但是我是这样理解的，像漫画一样。当其中的某一个片段出问题了，需要回复时候，我们可以通过快照，找到之前上面marker 拿到channel信息 重新计算，这样就可以保证分布式流动状态一致了。       </p>
</div><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By zhuyuping</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.0"></script><script src="/js/fancybox.js?version=1.6.0"></script><script src="/js/sidebar.js?version=1.6.0"></script><script src="/js/copy.js?version=1.6.0"></script><script src="/js/fireworks.js?version=1.6.0"></script><script src="/js/transition.js?version=1.6.0"></script><script src="/js/scroll.js?version=1.6.0"></script><script src="/js/head.js?version=1.6.0"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>