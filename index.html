<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="掌握java相关知识，擅长大数据，以及机器学习算法应用方面 java scala python"><meta name="keywords" content><meta name="author" content="zhuyuping"><meta name="copyright" content="zhuyuping"><title>朱遇平的github博客</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.0"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">zhuyuping</div><div class="author-info__description text-center">掌握java相关知识，擅长大数据，以及机器学习算法应用方面 java scala python</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">13</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">4</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">朱遇平的github博客</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">朱遇平的github博客</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/16/bazhanting/">拜占庭问题</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-16</time><div class="content"><p>昨天看到法国国庆7.14的飞行兵，可以自由的飞行，但是很多人想如果出现电子干扰，那飞行仪器被伪造假的指令或者被干扰烧坏，怎么保证飞行人员的安全呢？这其实跟导弹原理是一样的。其实也是一致性算法，只不过问题场景不一样。<br>我们在做架构时候，多台机器分布式一致性往往通过paxso raft 来保持多机器状态配置信息一致性，比如分布式存储用于维护文件目录metadata 配置信息，但是这些算法是在中心化无拜占庭节点情况才能成立的，在很多时候，比如导弹射击目标，被电子干扰可能电子硬件设备出错故障，不仅仅是故障，有时候还会做出错误的判断，还有在两将军问题，进攻一个地方，相互之间考信使（消息传递）但是消息传递过程中如果出现地方的间谍可能会不传递消息更有可能传递假的消息，这样的情况怎么保证达成共识呢？所以一般我们分这类问题为拜占庭问题，而上面讲的我们大多数中心化网络里面的raft paxso是非拜占庭问题。但是实际上无限制情况下在异步网络中不存在达到共识的算法，但是某些时候我们可以通过一些约束，拜占庭节点（间谍伪造节点）占总结点小于三分之1时候 是可以实现达到统一安全准确的共识。所以现在导弹发射就是多核心拜占庭错误下达到共识，确定实数目标，还有航天发射，还有现在的区块链去中心化网络，这些拜占庭共识算法往往用在这些领域。<br>常见的拜占庭算法有 随机共识 国王 女王 还有很多，后来<br>出现了一些改进，PBRT是比较出名的拜占庭算法，后来有些异步并行的PBRT算法，现在区块链在用它，另外随机共识 发展到后面 随机一些算法开始引入Pow 现在有些区块链币在用它。<br>PBRT算法 证明需要反证法，但是证明不重要，思路比较简单：大体是三次广播 实际上优化 可能要结合MST最小生成树还有最大独立子集还有BFS广度搜索来进行优化，另外过程可以并行异步化。源代码可以去参考一些开源的区块链框架搜索PBRT就可以看到。</p>
<p>PBFT（拜占庭容错）</p>
<p>基于拜占庭将军问题，一致性的确保主要分为这三个阶段：预准备（pre-prepare）、准备(prepare)和确认(commit)。流程如下图所示：</p>
<p>其中C为发送请求端，0123为服务端，3为宕机的服务端，具体步骤如下：</p>
<p>Request：请求端C发送请求到任意一节点，这里是0<br>Pre-Prepare：服务端0收到C的请求后进行广播，扩散至123<br>Prepare：123,收到后记录并再次广播，1-&gt;023，2-&gt;013，3因为宕机无法广播<br>Commit：0123节点在Prepare阶段，若收到超过一定数量的相同请求，则进入Commit阶段，广播Commit请求<br>5.Reply：0123节点在Commit阶段，若收到超过一定数量的相同请求，则对C进行反馈<br>详细可以参考这篇文章，网上也有很多模拟实现的源代码，但是实际上你最好参考一些区块链开源代码来实现，大大减少了时间复杂度。<br><a href="https://mp.weixin.qq.com/s/CE_92LZQUrH4OHNC_Doq2A" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/CE_92LZQUrH4OHNC_Doq2A</a><br>现在一些PBRT 也结合Raft Paxso 来实现Raft Paxso的PBRT实现，用于一些新的区块链应用。但是本质上海市PBRT和Raft结合。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/12/spark/">我对spark的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-12</time><div class="content"><p>我从2014年底接触spark开始，到后面15年初创业使用spark批量计算图像特征，图像算法，PCA等，LSH hash索引后，就开始对spark产生了浓厚的兴趣，spark 为什么这么高效，为什么spark 速度要比mapreduce快那么多，spark是怎么实现的，他为什么要这么做，他从发展初到现在2.4版本似乎本质的东西一直没有变过，后来我一直使用者，也一直研究看着他的源码设计，有了一些自己的心得体会，终于明白了上面的一些问题。<br>  首先spark 是迭代计算，所以适合那些数据块的处理，不适合那种算法里面各种参数迭代计算，那个需要BSP模型，首先我们spark 分布式计算系统，对于进程来说 无非就是 进程节点状态 与 消息 本地计算的关系，因为spark设计初衷是为了适合计算大批量的数据的问题，所以对于每一个进程节点来说不可能传输数据到每一个节点上进行操作，所以是 节点是数据，而计算是边，计算在节点之间传递的模型。对于我们每一个应用来说，因为是迭代计算，所以从数据输入 到 数据输出，所以他是一个DAG有向无环图，所以spark 的job 是构造成有向无环图的，而因为计算往节点上传输，所以需要有相关的DAG调度管理器，为了多机器并行并发计算所以需要进行任务分片所以肯定也有任务调度管理器，剩下的就是怎么保证数据获取 因为大数据块存储计算 所以需要一个分布式存储引擎，为了保证容错性，需要一致性算法或者机制来保证，为了高效内存计算，所以需要内存缓存管理机制，因为分布式计算系统，多机器，每个任务需要资源不同，所以同样需要进行资源管理机制来分片资源，多台机器上也需要进行相应的资源隔离，最后需要提供相关的编程接口转化为DAG，后来引入了schema,dataframe 引入了sql 以及引擎优化，扩展了相关第三方机器学习包，以及对流数据的支持。</p>
<p>  上面说了那么多，我们大概知道 spark需要有哪些组件。这些可以通过sparkContext sparkEnv可以看得到。比如上面提到的分布式存储，用于计算中间数据存储获取的 BlockManager 为了高速内存获取计算，所以肯定要BlockManager支持memstore diskstore,为了后续扩展使用spi接口，另外前面讲需要DAG解析的 ，对于这个来说我们肯定是在客户端提交任务到多台机器之前解析的，所以我们需要DagScheduler ,对于spark 这种分布式计算架构，因为他只有在map redurce suffle时候进行通讯，他的性能决定都在于shuffle过程 所以他通过DagScheduler分解窄依赖宽依赖，而对于数据源读取统一一个RDD结构，所以数据读取时候有partioner分区，同样因为调度系统 主从架构所以需要有相关RPC通讯 链接管理发起管理命令，启动后台守护，比如用于shuffle的 MapOutputTrackerMaster shufflerFetcher connectionManager ,其他的就是一些守护线程类，因为数据存储获取我们用blockManager来管理了，但是数据一致性完整性，需要一些其他手段了，storm使用了异或特性ack 来维护一个消息的一直完整，flink使用lamport的分布式快照算法来保证一致性，而spark通过lineage 血统来实现，为了能够通过血统恢复，所以RDD会记录父亲RDD的依赖信息，同时要保证RDD只读不变特性，对于每一个spark RDD 分布式弹性数据集结构来说首先blockManager可以维护多机器上数据顺序块存储分区管理，然后当每次迭代调用时候他提供了2中，一种是transformation 一种action 前者是用于RDD之间调用，后缀RDD转为实际值时候操作，所以对于每次迭代操作，都是一个RDD转为另外一个RDD过程，另外的RDD会记录下父亲RDD信息，所以你会发现从HadoopRDD/RDD创建后面的算子 转为MappedRDD ShuffleRDD MapPartionRDD等等过程，而每一个RDD里面执行相应的算子操作，其实就是一个scala函数闭包，转换过程，RDD.iterator 而获取这时候computeOrReadCheckPoint 从缓存本地内存来试着读取数据 ，执行最基本的容器数据迭代转换操作，但是这个过程中数据每次在线程里面不断的迭代处理，其实效率有点低，所以spark做了相关编译生成代码优化，其他都是怎么获取Block块，然后怎么shuffle 的过程，因为在前面我们从Job 到 DagSchuler到Stage 到 task 其实是pipeline手段以及task任务并行化手段，这过程中内存计算迭代 shuffle （Hash-base sort-Base）手段，为了保证多Job运行资源竞争问题，所以有DRF相关资源管理算法。 其实还有很多细节方面，后面为了支持SQL 在RDD算子 上面实现了SQL schema 结构dataFrame，为了能支持spark stream流计算，把流转为一个个Dstream最后代码转为spark 的RDD任务进行执行。<br>  所以总体来说 Spark源码分析可以从：<br>   1.分布式存储BlockManager 贯穿到RDD获取存储 shuffle获取存储 spark所有方方面面里面。<br>   2.网络通讯请求Rpc akka/Netty 涉及到后台调度时后台守护线程启动相关以及内部节点相互通讯。这里面有shuffleFetcher MapOutputTracker xxxBackend shuffleBlockManager值得一看<br>   3.DAG 计算step pipeline，任务分配 task分解 所以可以看看DagScheduler 以及 TaskScheduler<br>   4.数据一致性 RDD血统实现，checkPointer相关可以结合BlockManager 看看computeOrReadCheckPoint。<br>   大体就是 网络通讯 任务pipeline并行分片调度 资源管理 文件/内存存储  shuffle机制实现等等这些,这些就是spark 源码里面比较核心的一些组件了。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/08/kafka/">我了解的kafka</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-08</time><div class="content"><p>kafka 是个很好分布式消息中间件，但是我个人喜欢把他叫做流存储，下面我从我了解的地方说一下kafka，<br>kafka 架构其实很简单，producer consumer broker 其中broker consumer使用zookeeper 维护topic路径以及消费的相关位置信息。对于用户写有同步异步写，因为中心化系统所以 一主多从，用户发送时候多个队列批量累计异步发送 ack,也有同步ack,不同的是 主 不同步消息到从，而是由从主动从主这边拉取，这样任务可以并行化，同时为了保证C一致性，只有同步好的才能被消费者消费到，但是因为主 跟 从同步的日志记录位置差一次交互，由于存在2将军问题，消息丢失超时各种可能，所以需要使用LEO HW来进行保证读取消息日志同步一直性，这之间可能会出现宕机重启问题，所以为了保证选举一直是最新消息日志的机器，会记录ISR机器，只有ISR机器才能被当选主，后面就是一些常用的套路，分片副本，负载均衡，使用topic 作为namespace隔离 然后key partioner 来做分片，多个topic不同机器master 实现了简单负载均衡，而其中用户写入的消息都有一个唯一的id ,最后通过分片写入到相应的 topic_partion的目录里面，每一个目录里面会记录2个相关文件，每个文件以 offset.index offset.data 开头，比如000000000000001.index ,为了写的高吞吐高性能，数据先写入pagecache 然后定时fsync 或者缓冲到4k 刷新到文件里面，只有刷新到文件里面的才能被consumer消费。<br>用户当读取时候 通过zk获取到offset，然后通过二分查找或者skiplist找到相应的index 文件，为了索引文件快速查找一般加载到pachecache中，同时一些最近的日志也在pagecache中，通过找到相应的index文件，读取里面记录，index为了减少大小，一般采用整数压缩，只记录一些间隔的数据，比如 000000000005 比000000000001 间隔4 会记录 （4,197） 后面表示在offset.data的197位置，而同样他不会连续的记录，比如000000000006 记录可能不会记录（5，220）这条索引，他会等待数据达到4k或者一个间隔时间 记录下次来的新纪录，索引很稀疏。同样通过二分或者skiplist找到offset.data文件，读取相应的文件。里面解析格式，里面有校验码 时间戳 id key key length value valuelength等 这样读取到数据继续，通过zerocopy方式进行发送。实现高性能。<br>因为每一条记录只会追加到文件末尾，所以文件会越来越大，最大支持4G，大体就是这样。这就是我理解的kafka,他为什么这样实现，而不是他是这么实现，这才是我们学习最重要的。其实kafka 虽然我们叫消息中间件，其实很多方面跟分布式存储很相似，其实这些思想都是不变通用的，后面我有时间更新一下spark storm flink</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/07/08/bigData/">HDFS的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-07-08</time><div class="content"><p>其实HDFS是一个很了不起的分布式存储，虽然他有很多局限性，不适合小文件，并发能力不行，但是他的设计思路确实很多产品用来进行参考改进，以及实现更强大的产品。首先说下我是从下面几方面来看待他的：</p>
<p>   首先从设计架构 它元数据schema 与 实际数据进行分离，元数据存放在NameNode 数据数据存放在DataNode,同样为了HA 可以使用zookeeper或者joneralNode机制来实现高可用性，通用套路内存 +WAL 保持元数据恢复。从数据存储角度 分块block 三副本机制，传输类似http pipeline 流传输，通用为了防止丢失引入了chunk package简单校验码技术（其实后来改进有纠删码技术Raid技术），因为数据分块存储以及追加模式，后来很多（如hbase） 结合mvcc机制定时合并,mpp计算机制在上层实现更新删除以及计算操作，但是HDFS的问题在哪里呢？<br>   1.对于HDFS小文件问题 hdfs有很多改进的patch ,引入了小文件合并的服务以及客服端。<br>   2.对于冗余方面引入了raid纠错码技术<br>   3.对于存储瓶颈引入了加速技术，从数据组织和索引技术来加速有了carbondata parquent dataleak 有cube机制，从内存来技术有了alluxio以及tachyon<br>因为很多改进方案，可以让HDFS支持小文件，支持高并发，支持更新，支持容错高可用，支持方便扩展加速所以<br>现在很多技术使用计算存储分离中很多用HDFS作为底层存储，内存加速，计算层MPP SQL层SQL化来实现相关的系统框架。最重要的是他的设计架构，很多后来改进的系统之间服务通讯也很类似。如果你是一个做基础架构的，你会怎么来设计一个基础框架呢？其实看看HDFS hbase spark flink storm你就知道的，流程逻辑贯穿框架，组件角色定义骨骼，接口应用实现功能，是不是很像一个人。 其实后面我讲的spark storm flink yarn meso等调度框架 内部看像个人，外部看像生活中的管理。正是这样在美团一年多经历让我看清了原来业务架构与基础架构其实是万变不离其宗的。业务架构中，其实业务流程 到系统流程 到基础架构过程中，只是内在外在2个分形罢了。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/06/17/realtimeCompute/">我了解的大数据平台架构发展</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-06-17</time><div class="content"><p>2015年初，我进行创业，那时候storm实时计算开始流行。于是storm 实时计算 以及 spark 离线计算开始分别解决实时场景增量计算以及离线统计分析用于计算机视觉实现AR 图像识别方面。后来创业失败来到喜马拉雅平台架构，开发了分布式日志监控系统，分布式大数据平台，才明白原来这就是lambda架构，<br>将实时计算 批处理 以及 离线计算 以及服务层 分为多层，实时计算/微批处理用kafka 接入进行统计<br>计算写入，离线计算用kafka通过flume同步hdfs进行计算写入hbase es等其他。用反压以及markertime保证时效性，实时统计变量以及离线统计变量合并写入结果，hdfs、kafka进行presto BI相关分析，这一套架构很多年了，但是我认为的他有很多缺点，一方面是数据大量冗余存储各类es hbase redis hdfs kafka中以及数据丢失不一致CAP问题，还有一个缺点就是逻辑多套可能相同的，但是要部署几套逻辑，写几套代码。后来为了防止这个问题，我实现了一个sdk封装类似自己演示项目，通过stepbuilder模式 DAG封装 自动生成spark structstream代码 以及 sparkcore代码 进行了代码层的统一逻辑。不需要在流处理以及离线处理各写一套逻辑了。后来我发现了存在一些问题就是在做风控过程中，变量计算 spark流统计后计算的变量缓存在pika/redis里面，但是histroy data怎么合并,于是在想是否除了lamabda架构之外还有什么相关架构。后来了解到有kappa架构，他将就的是统一数据源，对于流处理以及离线处理统一从数据源中进行<br>生成相应的job，流处理写入中间存储缓存里面比如redis hbase中，离线存储写入hdfs 等永久存储里面，然后生成一套维护代码，定时的吧中间存储 合并到 离线存储中。这一套逻辑就是我上面演示项目做得，我试着封装统一API API生成相应的job代码，流计算代码，离线计算代码，然后中间存储与离线存储合并。用于股票分析。实时记录与历史记录，实时记录走flink、spark stream ,离线记录 spark ,代码通过自动生成统一逻辑层,元表管理层，然后在这SDK之上维护一个UI层以及整合机器学习层以及SQL层这是我那时候的打算。后来发现这种架构也有一些缺点。kappa缺点在于很难统一数据源，特别是流处理与离线处理，job的管理调度，元表管理，以及历史与中间缓存的合并。但是现今很多企业大数据平台架构大体都是这2种。<br>   前一段时间我在想难道没有一些新型的架构吗？后来去查询是否有解决统一数据源的分布式流存储系统，于是了解到了prevage ，新型的实时流存储系统，相比kafka来说，他分层存储，统一了上面说的流实时中间缓存存储问题，以及永久存储问题，数据无界以及永久存储问题，当然还有很多新特性，相当于解决kappa 的缺点，数据源统一、读写存储统一，大大减少了冗余，就跟flink 把所有的当做实时计算，微批处理只当做其中特例一样，于是一种flink/spark + prevage<br>架构开始出现。于是近期我在想自己空余学习项目是否需要进行改进扩展，我的打算是在prevage以及 flink之间实现一层包装层，从flink 每个算子以及机器学习各类操作，转为flink+prevage的操作代码，对用户透明，提供sql接口，这样大数据相当于实现一个类似的数据库，无论是实时流处理 微批处理 或者 离线处理，相当于数据库的表的操作处理。平台既是数据库，屏蔽了所有大数据离线实时各类计算不一致问题。对外提供查询 机器学习训练预测功能。</p>
<p>未来5G开始流行，物联网设备，大量数据产生，实时性流计算，实时预警感应越发重要，大数据方面个人比较看好分布式流存储，以及 去中心化分布式计算 ，去中心化流式云存储问题。一直在研究这个方面。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/06/05/NAS研究/">NAS 我眼中的AutoML</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-06-05</time><div class="content"><p>我在空余时间做的一个展示项目其实核心并没有完成，特别是对于一个DAG有向无环图来说怎么让机器自动找到一个最优的DAG才是最核心的关键内容，最近google新出的efficentNet模型就是基于NAS的automl得到的模型作为baseline然后 动态优化相关的深度精度宽度来实现更高的基准，所以最近这段时间个人一直在看NAS，一方面分布式计算是否可以用NAS来优化，另一方面automl 实现NAS 详细大概是怎么做的呢？</p>
<p>说到automl 我们要了解做算法 做大数据大体流程，<br>数据接入读取，数据清洗转换，数据特征提取，特征选择，数据模型训练可视化，超参数优化 这几个步骤，更加进一步就是automl 也就是超参数优化自动化，模型生成自动化，进而拓扑图生成自动化， 但是自动化要解决的问题就是怎么样生成最优的一个拓扑结构呢？</p>
<p>我了解最近的研究大体分为下面几个方面：<br>首先目标是 寻找最优的拓扑结构，寻找最优的超参数<br>对于每一个节点来说节点之间怎么连接组合，对于最优结构来说可以从图论考虑，寻找最优子图，可以从系列考虑，使用RNN LSTM Transformer来一层层生成,对于每一种结构的超参数就是搜索优化问题，这样可以把结构以及参数作为网络的输出，进行生成。</p>
<p>强化学习：</p>
<p>我以前在进行NAS尝试，可以效果不理想，还有一些问题没解决，还在研究，那时候使用的是强化学习，通过训练生成相应的子网络，子网络使用校验数据进行预测的精度MSE等加权作为Reward进行反馈，从而进一步调整，子网络，而每一步节点或者相应的结构块block cell使用RNN生成，超参数另外作为一个输出，输出多个参数，但是当时块与各个层之间连接太复杂了没有解决这类问题。</p>
<p>同样只要了解到NAS 目标是寻找一个最优子结构，最优超参数问题，所以实际上是一个最优化问题，所以可以用贝叶斯 遗传算法 梯度算法 线性规划等等。</p>
<p>最近在研究分布式算法，一直想NAS 是否可以用来解决分布式算法的某些问题这应该是一个有趣的问题？而且自己还没想通某些网络结构比较复杂，怎么整合到NAS常用的卷积网络，全连接，激活函数，链接结构，RNN结构，LSTM结构，这几类比较简单整合，但是transformer以及GAN 以及 VAE 这类改怎么整合呢？</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/20/logicTime/">谈谈我对逻辑时间的理解</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-20</time><div class="content"><p>在分布式一致性算法中接触到了逻辑时间概念，逻辑时钟概念被用于分布式快照算法以及分布式一致性算法，比如上面的lamport快照算法以及亚马逊的dynamic分布式存储。比如vector clock<br>在分布式系统中，我们无法保证全局的一个统一时钟，虽然有NTP 相关保证ms内的时间同步，但是在高并发进程消息通讯中 由于消息丢失，超时各类问题，以及拜占庭问题，消息传输会导致乱系 丢失 篡改 重复传送，比如在storm中 算子代表节点，边代表处理的数据消息，消息进程先后顺序无法用全局时间戳来进行表达，所以出现了逻辑时钟的概念，lamport 1978年提出。</p>
<p>  对于多个进程之间，如果2个进程之间，A发送消息到B ，那么必然有A B之间的依赖，也就是说 A的时间必然小于B的时间。存在因果依赖关系，逻辑时间有三种方式，标量时钟 向量时钟 矩阵时钟，但是这不重要，因为进程之间，无非是内部事件操作，发送，接受，所以我们要保证的是当执行操作时候本地时钟的变化，以及发送时候 接受时候 时钟改怎么记录更新全局时钟。所以有2个规则：<br>  A规则：<br>    当进程执行本地操作事件，发送事件，接受事件时候本地时钟该如何变化。<br>  B规则：<br>    当进程发送接受时候 本地时钟以及全局时钟如何更新。</p>
<p><strong>标量时钟</strong>：就是通过一个标量记录每一个进程来记录全局时钟以及本地时钟。<br> A。 当用户执行发送接受以及本地事件之前进行本地时钟更新， 本地时钟进行+N操作<br> N一般取1。<br>B。 当用户发送事件时候携带 本地时钟c 到消息中，当另外进程收到消息时候，获取消息中携带的本地时钟c 进行更新<br>改进程本地时钟更新为Max(c，本地时钟)，因为是接受事件，所以符合规则A定义，所以再执行规则A。</p>
<p><strong>向量时钟：</strong><br>向量时钟就是通过一个向量记录1个进程看到的所有其他进程包括自己的时钟状态。<br>比如V（i） 表示 低I个进程的时钟状态。假如进程数为N 那么，每一个进程通过消息传递因果关系得到的信息，记录着自己所知道的各个进程的本地时钟。</p>
<p>A：<br>  当进程I执行相关事件时候<br>  V（i）= V（i）+N<br>B:当发送事件时候 携带V发送到消息M中当进程收到消息时候，获取消息M中的V，并更新V所有的值为本地事件和V的最大值。因为是接受事件，所以符合规则A定义，所以再执行规则A。</p>
<p>矩阵时钟也类似。</p>
<p>大体是这样，一般很多文章都会统一说vector clock 大体一个意思.</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/17/distributeStore/">分布式的一些想法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-17</time><div class="content"><p>接触过很多分布式系统，以及内部各种设计，想分布式管理系统，分布式计算系统，分布式存储/数据库 ，大体市场上的都可以分到这几类，负载均衡有一套体系，自动扩容有一套体系，限流，分配 副本 一致性都有专门的相关算法知识，头几年我那时候很困惑，这些系统到底为啥要这样做，架构架构什么才是架构的目的，用这么多算法理论，为什么要这么做，最终是为了什么？大体我总结到的原因是这么几类：</p>
<p><strong>外层来看：</strong></p>
<ul>
<li>1。从业务流程抽象到系统流程并架构化。主要用来解决通用业务问题以及后续的扩展问题。</li>
<li>2。开发职能分离单一化，通用化，易管理扩展以便能够系统之间开发维护故障影响依赖解耦。主要是外层的一些设计思想SOA 微服务 servermesh网格</li>
</ul>
<p>这一类是大体是 从产品业务理解，项目管理 ，开发效率，项目维护迭代等方面做出的一些改进的各类思想。就像人的外貌 体格 身高。</p>
<p><strong>内层来看：</strong></p>
<ol>
<li>性能以及瓶颈能否支持业务以及后续发展需要。所以各种无锁 pageCache 内存 异步 新的结构等等机制出来解决这类问题</li>
<li>可伸缩性，就是时间，效率，性能是否支持横向纵向的扩展，所以各类分片 负载均衡 调度管理算法出来解决这类问题。</li>
<li>灵活性 就是产品业务方面能否适应后续的改动变化，拆分 组合 所以各类设计模式以及思想出来了解决这类问题。</li>
<li>可扩展性，自动化 就是一些通用的东西，能否适应后续业务产品需要，方便开发扩展适配，所以各类产品业务都有一个最终提炼的思想核心，每个人根据他对产品业务了解来设计。</li>
<li>后面的就是CAP BASE问题 怎么保证系统可用可靠性 ，故障转移，多活 ，自动扩容，备份容灾恢复，负债均衡，点对点，还是主从，还是读写分离。怎么保证数据一致，在这中间怎么取舍，各种异常情况可能性都需要考虑。</li>
<li>可维护编排部署管理性，主要从易用，测试，管理 维护角度来考虑</li>
<li><p>最后就是一些通用的需要的UI 监控管理层。这个可有可没有</p>
<p>这一类就像是 人的本质。</p>
<p>最近在研究一些东西，一方面像自己实现一个分布式计算的应该怎么做？自己实现一个分布式存储该怎么做？我要考虑哪些方面，要怎么改进，要怎么做，另外一方面自己 不用一些库能否写出来相关算法。这是我后面要一直成长研究的方向。还有很多陌生的知识需要去工作生活空余了解研究。 就像我用了很多图的思想，但是图从最开始的基本图的bfs dfs 一些基本算法，到图中基于一些特性的图搜索查找，挖掘的图算法，到最新的图嵌入，到图深度学习，到其中一个分支 知识图谱。可谓是博大精深。但是每天进步一点点，最后总会厚积薄发!</p>
</li>
</ol>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/15/distributeChandy-Lamport/">我理解的分布式快照算法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-15</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/chandy-lamport快照算法/">chandy-lamport快照算法</a></span><div class="content"><p> 对于分布式计算系统，分布式系统数据流动变化时候，往往需要记录一个全局状态也就是快照，以便系统崩溃时候能够通过快照得到每个节点本地快照并恢复。想象一个场景，城市交通四通八达，有很多下水道，蚂蚁是怎么找到并记录道路状况的呢？每一条道路派一支蚂蚁去探路，那如果我们要拍一张照片，全局的快照，但是我们一次性拍不下全部，那我们怎么做呢？我们就每一条分支路径拍一张快照，那么所有的本地快照就可以组成一张全局快照，那每一张本地快照改记录什么呢？对于分布式系统，有是数据从边流向节点，或者操作从边流向节点，进程之间也是一样，那么怎样保证记录快照呢？ 我们这样想，对于当前节点状态我们肯定是要记录的，对于当前节点输入的也是要记录的，输出呢？不需要，因为我们可以通过节点状态以及输入可以得到输出，所以只需要记录状态以及输入就行，但是输入什么时候该记录呢，我们拍照肯定是拍照点之前快到拍照这个点的数据，就像漫画里面一个个慢动作，最后我们眼中看到的是慢动作组成的动作，但是我们快照就是一个个慢动作的帧，是我们最后眼镜拍照那个状态之前的帧<br>    下面说一下<br>     <strong>Chandy Lamport 算法<br>这个算法把P进程-&gt;Q进程数据交互分为四个部分，其中因为每个节点时间上的不一致，所以采用了一个marker 来记录什么时候该拍每一个节点的快照，所以分为4个过程。也就是2个初始态 2个中间态，</strong></p>
<p>如果当进程拥有marker状态记为s1,进程未拥有token状态记为s0。<br>1.P进程开始发送marker 到Q进程 此时 P Q  应该为[ p：s1 q：s0] 这时候相当于漫画我们开始要画第一帧，此时人物 环境状态 一张静态图。<br>2.P已经发送了marker了在传输的channel边上 即将到Q进程<br>这时候 P Q 应该为 [ p：s0 q：s0 ] 哈哈，此时想象变化的是marker的channel吧，这时候相当于漫画我们开始画第二帧 第二帧 第几帧的中间过程了。多张动画帧<br>3.进程Q收到marker marker已经到了Q ，这时候相当于我们动画里面 一个片段。 [ p：s0 q：s1 ]<br>如果有双向的场景，我们倒退场景，顺序 逆序<br>4.这时候进程Q 发送marker到P<br>同样有[ p：s0 q：s0 ]<br>5.这时候进程P 收到了来自Q的marker 同样有 [ p：s1 q：s0 ]<br>那么他怎么快照呢？<br><strong>实际过程很像： 漫画从手绘到动画的过程</strong>。</p>
<p><strong>我们要快照时候相当于画动画初始帧，但是这个帧有很多后续变化，所以<br>第一步 先记录初始帧状态，然后当记录成功后发送一个marker拍快照的标记给后续所有帧，<br>这时候遇到后续帧会有如下情况，<br>如果后续动画帧 我们没有记录下来，那么我们要记录当前这帧，同时，我们知道这之前的所有帧已经记录好了，如果实际上发现这一帧已经记录了，说明后续动画有从当前帧开始变化了，下一个片段了，这时候是不是要记录从从上一个片段记录帧完成后 到 这个片段开始记录帧marker之前所有的帧，也就是channel信息</strong></p>
<p>下面是原论文信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//（1）（2）</span><br><span class="line">begin </span><br><span class="line">         p  records its state;</span><br><span class="line">end    </span><br><span class="line">then </span><br><span class="line">        p sends one marker along c after p records its state and before p sends further messages along c.     </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//（3）（4） </span><br><span class="line">if q has not recorded its state then </span><br><span class="line">      begin </span><br><span class="line">                 q records its state; </span><br><span class="line">                 q records the state c as the empty sequence </span><br><span class="line">      end </span><br><span class="line">else </span><br><span class="line">       q records the state of c as the sequence of messages received along c after q’s state was recorded and before q received the marker along c.</span><br></pre></td></tr></table></figure>
<p>晦涩难懂但是我是这样理解的，像漫画一样。当其中的某一个片段出问题了，需要回复时候，我们可以通过快照，找到之前上面marker 拿到channel信息 重新计算，这样就可以保证分布式流动状态一致了。<br>其实所有的这些是为了保证2个原则：<br><strong>在本地快照之前：</strong><br>  <strong>进程发送的任何消息一定要记录到全局快照里面</strong></p>
<p><strong>本地快照之后：</strong><br>  <strong>进程发送的任何消息一定不能记录到全局快照里面</strong></p>
<p><strong>所以 从发送的角度考虑<br>从接受的角度考虑，从节点考虑，从channel 考虑。来保证上面2个原则就是其精髓。</strong></p>
<p>所以你会看到 P ===&gt; Q<br>marker是为了区分发送前的消息与发送后的消息，1，2过程为了保证  <strong>在本地快照之前：</strong> 3,4过程是为了 保证 <strong>本地快照之后：</strong> </p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2019/05/09/spark理解/">我理解的分布式计算系统</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-09</time><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/spark-storm-flink/">spark storm flink</a></span><div class="content"><p>接触spark从2015年出现火爆开始到至今。其中接触并使用过storm 使用过现今比较流行的flink，还记得2014年底，我在一家医疗分析的创业公司时候，工作需要，研究并接触到了并发批处理，那时候mapreduce开始出现，接触到了disrupter并发框架，接触到了netty网络通讯一些东西，那时候想是不是通过netty网络通讯保持多台机器并发并行的执行并发框架是不是可以实现分布式计算呢，那是我第一个初步的理解，后来从写代码到熟悉api 到了解一些细节。并心中对spark storm 到 flink 一些归纳总结思考后，我提炼了一些自己的认识。下面说说我眼中的分布式计算。<br>   在传统高并发 有多线程  多进程，比如多线程共享内存方式，多进程消息传递方式，到充分利用特性，多进程并行，多线程消息传递机制，进而到分布式多机器多进程多线程多协程，所以出现了各类分布式计算框架，我总结的大体分类几类：第一类 是基于mapreduce 算子拓扑图的 比如mapreduce storm flink spark 第二类 基于通讯机制 消息之间或实现共享内存共享线程等 ，通讯调度计算的 这一类有ignite xgboost ps-servier 参数服务器这些 第三类基于 图 节点传播pregel模式的 ，大体从数据来说 可以从图的 顶点和边来考虑。一类数据比较小 所以数据是边 顶点是算子 比如storm flink,一类数据较大 所以数据是顶点，算子是边。<br>   大体来说 现在第一类算子拓扑图的分布式计算大体模式是通过抽象出操作作为一些算子，算子之上封装成一些table schema dataset sql一些抽象，对于底层算子之间 只有shuffle 才会进行通讯其他之间不通讯，而算子之间通过有向无环图DAG进行管理划分，对于每一个Job 大体都是 分布式存储抽象层<br>任务调度分配层 网络通讯层 资源管理隔离层，动态跟踪监控层，然后为了保证一致性 通过分布式快照算法来实现流程数据一致性流程完整性。这个设计思想 在其他分布式存储 分布式中间件中很相似也有异曲同工之妙。最近看到flink 开始采用akka 消息通讯机制来替代原来的共享内存锁机制，如果后面flinkml需要支持深度学习的化，也许flink akka实现会向第一类allreduce机制来兼容。</p>
</div><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2019 By zhuyuping</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.0"></script><script src="/js/fancybox.js?version=1.6.0"></script><script src="/js/sidebar.js?version=1.6.0"></script><script src="/js/copy.js?version=1.6.0"></script><script src="/js/fireworks.js?version=1.6.0"></script><script src="/js/transition.js?version=1.6.0"></script><script src="/js/scroll.js?version=1.6.0"></script><script src="/js/head.js?version=1.6.0"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script></body></html>